{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b6fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3c0cb",
   "metadata": {},
   "source": [
    "### First, we load the titatic dataset and convert all categorical variables to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6d93d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived        Age  SibSp  Parch     Fare  Pclass_1   \n",
       "0              1         0  22.000000      1      0   7.2500     False  \\\n",
       "1              2         1  38.000000      1      0  71.2833      True   \n",
       "2              3         1  26.000000      0      0   7.9250     False   \n",
       "3              4         1  35.000000      1      0  53.1000      True   \n",
       "4              5         0  35.000000      0      0   8.0500     False   \n",
       "..           ...       ...        ...    ...    ...      ...       ...   \n",
       "886          887         0  27.000000      0      0  13.0000     False   \n",
       "887          888         1  19.000000      0      0  30.0000      True   \n",
       "888          889         0  29.699118      1      2  23.4500     False   \n",
       "889          890         1  26.000000      0      0  30.0000      True   \n",
       "890          891         0  32.000000      0      0   7.7500     False   \n",
       "\n",
       "     Pclass_2  Pclass_3  Sex_female  Sex_male  Embarked_C  Embarked_Q   \n",
       "0       False      True       False      True       False       False  \\\n",
       "1       False     False        True     False        True       False   \n",
       "2       False      True        True     False       False       False   \n",
       "3       False     False        True     False       False       False   \n",
       "4       False      True       False      True       False       False   \n",
       "..        ...       ...         ...       ...         ...         ...   \n",
       "886      True     False       False      True       False       False   \n",
       "887     False     False        True     False       False       False   \n",
       "888     False      True        True     False       False       False   \n",
       "889     False     False       False      True        True       False   \n",
       "890     False      True       False      True       False        True   \n",
       "\n",
       "     Embarked_S  \n",
       "0          True  \n",
       "1         False  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  \n",
       "..          ...  \n",
       "886        True  \n",
       "887        True  \n",
       "888        True  \n",
       "889       False  \n",
       "890       False  \n",
       "\n",
       "[891 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv('titanic.csv')\n",
    "data_frame = data_frame.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "data_frame = pd.get_dummies(data_frame, columns=['Pclass', 'Sex', 'Embarked'])\n",
    "data_frame = data_frame.fillna(data_frame.mean())\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d66e7",
   "metadata": {},
   "source": [
    "### Next, we convert the dataset to a NumPy array and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9dcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_frame[['Survived']].to_numpy().astype(float)\n",
    "observ = data_frame.drop(['PassengerId', 'Survived'], axis=1).to_numpy().astype(float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(observ, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77af664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea47c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to normalize Age and Fare, so columns 0,3\n",
    "age_min, age_max = X_train[:,0].min(), X_train[:,0].max()\n",
    "fare_min, fare_max = X_train[:,3].min(), X_train[:,3].max()\n",
    "# Apply\n",
    "X_train[:,0] = (X_train[:,0] - age_min) / (age_max - age_min + 1e-5)\n",
    "X_test[:,0] = (X_test[:,0] - age_min) / (age_max - age_min + 1e-5)\n",
    "X_train[:,3] = (X_train[:,3] - fare_min) / (fare_max - fare_min + 1e-5)\n",
    "X_test[:,3] = (X_test[:,3] - fare_min) / (fare_max - fare_min + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05a1c9",
   "metadata": {},
   "source": [
    "### Now, let's create our sequential model with four linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ccd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "first_model = [\n",
    "    nn.Linear(X_train.shape[1], 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8526e",
   "metadata": {},
   "source": [
    "### We can already apply it to our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d54baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(X_test[:20], dtype=torch.float)\n",
    "for layer in first_model:\n",
    "    inp = layer(inp)\n",
    "#first_model(torch.tensor(X_test[:20], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0cef270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4971],\n",
       "        [0.4768],\n",
       "        [0.4897],\n",
       "        [0.4774],\n",
       "        [0.4728],\n",
       "        [0.4863],\n",
       "        [0.4612],\n",
       "        [0.4299],\n",
       "        [0.4836],\n",
       "        [0.5055],\n",
       "        [0.4854],\n",
       "        [0.4808],\n",
       "        [0.4626],\n",
       "        [0.4943],\n",
       "        [0.4635],\n",
       "        [0.4824],\n",
       "        [0.4464],\n",
       "        [0.4984],\n",
       "        [0.4715],\n",
       "        [0.4826]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67138f74",
   "metadata": {},
   "source": [
    "### Now, let's fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8973e8",
   "metadata": {},
   "source": [
    "### As you may have seen, this does not work. We have to implement the training ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b345d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLPModel(nn.Module):\n",
    "    def __init__(self, input, *hidden_layers, lr=0.1, dropout=0.2):\n",
    "        super().__init__() # <- Very important!\n",
    "        self.lr = lr\n",
    "        ## Build model\n",
    "        n_neurons = [input] + list(hidden_layers)\n",
    "        self.layers = []\n",
    "        for i, o in zip(n_neurons[:-1], n_neurons[1:]):\n",
    "            self.layers += [\n",
    "                nn.Linear(i, o),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ]\n",
    "        self.layers += [\n",
    "            nn.Linear(hidden_layers[-1], 1), # Output layer with no final activation\n",
    "            nn.Sigmoid(), # Final activation function\n",
    "        ]\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers) # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    def predict(self, X, th=0.5):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X)\n",
    "        return (y_hat >= th).float()\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        y_hat = self(X)\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "        \n",
    "    def validation_step(self, X, y):\n",
    "        with torch.no_grad():\n",
    "            return self.train_step(X, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid, epochs=10):\n",
    "        ## Convert dataset\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "        \n",
    "        X_valid = torch.tensor(X_valid, dtype=torch.float)\n",
    "        y_valid = torch.tensor(y_valid, dtype=torch.float)\n",
    "        \n",
    "        ## Load Optimizer\n",
    "        optimizer = self.configure_optimizers()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'{epoch+1}/{epochs}:')\n",
    "            # Training\n",
    "            self.train() # Set model to training mode\n",
    "            optimizer.zero_grad(set_to_none=True) # Sets all gradients to Zero (Default is to None) \n",
    "            loss = self.train_step(X_train, y_train) # Execute Forward pass and calculate Loss\n",
    "            loss.backward() # Execute Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "            self.eval() # Set model to validation mode\n",
    "            \n",
    "            # Validation\n",
    "            loss_valid = self.validation_step(X_valid, y_valid)\n",
    "            print(f'Training Loss: {loss.item():1.4f}', f'Validation Loss: {loss_valid.item():1.4f}')\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad180e6",
   "metadata": {},
   "source": [
    "### Create a model similar to the one before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8683b5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLPModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "    (15): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (16): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model = MyMLPModel(X_train.shape[1], 32, 16, 8, 4)\n",
    "second_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb9ec39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.predict(X_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a151d8",
   "metadata": {},
   "source": [
    "### Train it and calculate the test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ae2b864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000:\n",
      "Training Loss: 0.6773 Validation Loss: 0.6799\n",
      "2/1000:\n",
      "Training Loss: 0.6764 Validation Loss: 0.6793\n",
      "3/1000:\n",
      "Training Loss: 0.6756 Validation Loss: 0.6788\n",
      "4/1000:\n",
      "Training Loss: 0.6749 Validation Loss: 0.6783\n",
      "5/1000:\n",
      "Training Loss: 0.6743 Validation Loss: 0.6778\n",
      "6/1000:\n",
      "Training Loss: 0.6741 Validation Loss: 0.6774\n",
      "7/1000:\n",
      "Training Loss: 0.6732 Validation Loss: 0.6770\n",
      "8/1000:\n",
      "Training Loss: 0.6726 Validation Loss: 0.6767\n",
      "9/1000:\n",
      "Training Loss: 0.6725 Validation Loss: 0.6763\n",
      "10/1000:\n",
      "Training Loss: 0.6716 Validation Loss: 0.6760\n",
      "11/1000:\n",
      "Training Loss: 0.6711 Validation Loss: 0.6757\n",
      "12/1000:\n",
      "Training Loss: 0.6707 Validation Loss: 0.6754\n",
      "13/1000:\n",
      "Training Loss: 0.6701 Validation Loss: 0.6752\n",
      "14/1000:\n",
      "Training Loss: 0.6695 Validation Loss: 0.6749\n",
      "15/1000:\n",
      "Training Loss: 0.6696 Validation Loss: 0.6747\n",
      "16/1000:\n",
      "Training Loss: 0.6693 Validation Loss: 0.6745\n",
      "17/1000:\n",
      "Training Loss: 0.6685 Validation Loss: 0.6743\n",
      "18/1000:\n",
      "Training Loss: 0.6683 Validation Loss: 0.6742\n",
      "19/1000:\n",
      "Training Loss: 0.6682 Validation Loss: 0.6740\n",
      "20/1000:\n",
      "Training Loss: 0.6675 Validation Loss: 0.6739\n",
      "21/1000:\n",
      "Training Loss: 0.6675 Validation Loss: 0.6737\n",
      "22/1000:\n",
      "Training Loss: 0.6677 Validation Loss: 0.6736\n",
      "23/1000:\n",
      "Training Loss: 0.6665 Validation Loss: 0.6735\n",
      "24/1000:\n",
      "Training Loss: 0.6669 Validation Loss: 0.6734\n",
      "25/1000:\n",
      "Training Loss: 0.6669 Validation Loss: 0.6733\n",
      "26/1000:\n",
      "Training Loss: 0.6664 Validation Loss: 0.6732\n",
      "27/1000:\n",
      "Training Loss: 0.6657 Validation Loss: 0.6731\n",
      "28/1000:\n",
      "Training Loss: 0.6660 Validation Loss: 0.6730\n",
      "29/1000:\n",
      "Training Loss: 0.6656 Validation Loss: 0.6730\n",
      "30/1000:\n",
      "Training Loss: 0.6654 Validation Loss: 0.6729\n",
      "31/1000:\n",
      "Training Loss: 0.6653 Validation Loss: 0.6729\n",
      "32/1000:\n",
      "Training Loss: 0.6661 Validation Loss: 0.6728\n",
      "33/1000:\n",
      "Training Loss: 0.6654 Validation Loss: 0.6728\n",
      "34/1000:\n",
      "Training Loss: 0.6647 Validation Loss: 0.6728\n",
      "35/1000:\n",
      "Training Loss: 0.6655 Validation Loss: 0.6727\n",
      "36/1000:\n",
      "Training Loss: 0.6653 Validation Loss: 0.6727\n",
      "37/1000:\n",
      "Training Loss: 0.6647 Validation Loss: 0.6727\n",
      "38/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6727\n",
      "39/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6726\n",
      "40/1000:\n",
      "Training Loss: 0.6649 Validation Loss: 0.6726\n",
      "41/1000:\n",
      "Training Loss: 0.6646 Validation Loss: 0.6726\n",
      "42/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6726\n",
      "43/1000:\n",
      "Training Loss: 0.6649 Validation Loss: 0.6726\n",
      "44/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6726\n",
      "45/1000:\n",
      "Training Loss: 0.6653 Validation Loss: 0.6726\n",
      "46/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6726\n",
      "47/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "48/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "49/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6726\n",
      "50/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "51/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6726\n",
      "52/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6726\n",
      "53/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "54/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6726\n",
      "55/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6726\n",
      "56/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6726\n",
      "57/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6726\n",
      "58/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "59/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6726\n",
      "60/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6726\n",
      "61/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6727\n",
      "62/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6727\n",
      "63/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6727\n",
      "64/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6727\n",
      "65/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6727\n",
      "66/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6727\n",
      "67/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6727\n",
      "68/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6727\n",
      "69/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6727\n",
      "70/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6727\n",
      "71/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6728\n",
      "72/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6728\n",
      "73/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6728\n",
      "74/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6728\n",
      "75/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6728\n",
      "76/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6728\n",
      "77/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6728\n",
      "78/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6728\n",
      "79/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6729\n",
      "80/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6729\n",
      "81/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6729\n",
      "82/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6729\n",
      "83/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6729\n",
      "84/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6729\n",
      "85/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6729\n",
      "86/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6729\n",
      "87/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6729\n",
      "88/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6729\n",
      "89/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6729\n",
      "90/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "91/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6730\n",
      "92/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6730\n",
      "93/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6730\n",
      "94/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6730\n",
      "95/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6730\n",
      "96/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6730\n",
      "97/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "98/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "99/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "100/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "101/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "102/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6731\n",
      "103/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "104/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "105/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "106/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "107/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6731\n",
      "108/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "109/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "110/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6731\n",
      "111/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6731\n",
      "112/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "113/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6731\n",
      "114/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "115/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "116/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "117/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "118/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "119/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "120/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6732\n",
      "121/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "122/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "123/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "124/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "125/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "126/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "127/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "128/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "129/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6732\n",
      "130/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6732\n",
      "131/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "132/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "133/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "134/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "135/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "136/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "137/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "138/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6733\n",
      "139/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "140/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "141/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "142/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "143/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "144/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "145/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "146/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6733\n",
      "147/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "148/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "149/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "150/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6733\n",
      "151/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "152/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "153/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "154/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "155/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "156/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "157/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "158/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "159/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "160/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "161/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "162/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "163/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "164/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "165/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "166/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "167/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6733\n",
      "168/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "169/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "170/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "171/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "172/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "173/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "174/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "175/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "176/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "177/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "178/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "179/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "180/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "181/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "182/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "183/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "184/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "185/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "186/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "187/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "188/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6733\n",
      "189/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6733\n",
      "190/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "191/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "192/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "193/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "194/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "195/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "196/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "197/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "198/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "199/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "200/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "201/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6733\n",
      "202/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "203/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "204/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "205/1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "206/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6733\n",
      "207/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6733\n",
      "208/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "209/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "210/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "211/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "212/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "213/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "214/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "215/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "216/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "217/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "218/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "219/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "220/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "221/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6733\n",
      "222/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "223/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "224/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "225/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "226/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "227/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "228/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6733\n",
      "229/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "230/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "231/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "232/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6733\n",
      "233/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "234/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "235/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6733\n",
      "236/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "237/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "238/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "239/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "240/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "241/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "242/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "243/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "244/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "245/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "246/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "247/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "248/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "249/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6733\n",
      "250/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6733\n",
      "251/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "252/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "253/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "254/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "255/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6733\n",
      "256/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "257/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6733\n",
      "258/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "259/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "260/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6733\n",
      "261/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "262/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "263/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "264/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "265/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "266/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "267/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6733\n",
      "268/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6733\n",
      "269/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "270/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6733\n",
      "271/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6733\n",
      "272/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6733\n",
      "273/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6733\n",
      "274/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "275/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6733\n",
      "276/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "277/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "278/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "279/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6733\n",
      "280/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6733\n",
      "281/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6733\n",
      "282/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6733\n",
      "283/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6733\n",
      "284/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6733\n",
      "285/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6733\n",
      "286/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6733\n",
      "287/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6733\n",
      "288/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6732\n",
      "289/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "290/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "291/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "292/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "293/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "294/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6732\n",
      "295/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "296/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6732\n",
      "297/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "298/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6732\n",
      "299/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "300/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "301/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "302/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "303/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "304/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "305/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "306/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "307/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "308/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6732\n",
      "309/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "310/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "311/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "312/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6732\n",
      "313/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "314/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6732\n",
      "315/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "316/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "317/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "318/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6732\n",
      "319/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "320/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6732\n",
      "321/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "322/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "323/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "324/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "325/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "326/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6732\n",
      "327/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "328/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "329/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "330/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "331/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6732\n",
      "332/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6732\n",
      "333/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "334/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "335/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "336/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6732\n",
      "337/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "338/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "339/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "340/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "341/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "342/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "343/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "344/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6732\n",
      "345/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6732\n",
      "346/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "347/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "348/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "349/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "350/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "351/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "352/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "353/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "354/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "355/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "356/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "357/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "358/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "359/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6732\n",
      "360/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "361/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "362/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "363/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "364/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6732\n",
      "365/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "366/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "367/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6732\n",
      "368/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "369/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "370/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "371/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "372/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6732\n",
      "373/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "374/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6732\n",
      "375/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6732\n",
      "376/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6732\n",
      "377/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6732\n",
      "378/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "379/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6732\n",
      "380/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "381/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6732\n",
      "382/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6732\n",
      "383/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "384/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6732\n",
      "385/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6732\n",
      "386/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "387/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "388/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6732\n",
      "389/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6732\n",
      "390/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6732\n",
      "391/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6732\n",
      "392/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6731\n",
      "393/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6731\n",
      "394/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6731\n",
      "395/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "396/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "397/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6731\n",
      "398/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "399/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6731\n",
      "400/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6731\n",
      "401/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "402/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6731\n",
      "403/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "404/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "405/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "406/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "407/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "408/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "409/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6731\n",
      "410/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6731\n",
      "411/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6731\n",
      "412/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6731\n",
      "413/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6731\n",
      "414/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "415/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "416/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "417/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "418/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "419/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6731\n",
      "420/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "421/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "422/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "423/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "424/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6731\n",
      "425/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "426/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6731\n",
      "427/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6731\n",
      "428/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "429/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "430/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "431/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "432/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "433/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6731\n",
      "434/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "435/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6731\n",
      "436/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "437/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6731\n",
      "438/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "439/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "440/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6731\n",
      "441/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "442/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6731\n",
      "443/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6731\n",
      "444/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6731\n",
      "445/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6731\n",
      "446/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6731\n",
      "447/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "448/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6731\n",
      "449/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6731\n",
      "450/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6730\n",
      "451/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6731\n",
      "452/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6731\n",
      "453/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6731\n",
      "454/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6730\n",
      "455/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6730\n",
      "456/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "457/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6730\n",
      "458/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "459/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6730\n",
      "460/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6730\n",
      "461/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "462/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6730\n",
      "463/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "464/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6730\n",
      "465/1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6630 Validation Loss: 0.6730\n",
      "466/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6730\n",
      "467/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6730\n",
      "468/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "469/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6730\n",
      "470/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6730\n",
      "471/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6730\n",
      "472/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6730\n",
      "473/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "474/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "475/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "476/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "477/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6730\n",
      "478/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6730\n",
      "479/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6730\n",
      "480/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6730\n",
      "481/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6730\n",
      "482/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "483/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6730\n",
      "484/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "485/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6730\n",
      "486/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "487/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "488/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "489/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6730\n",
      "490/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6730\n",
      "491/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "492/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6730\n",
      "493/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "494/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "495/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6730\n",
      "496/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6730\n",
      "497/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "498/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "499/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "500/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6730\n",
      "501/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6730\n",
      "502/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6730\n",
      "503/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6730\n",
      "504/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "505/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6730\n",
      "506/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6730\n",
      "507/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6730\n",
      "508/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6730\n",
      "509/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6730\n",
      "510/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6730\n",
      "511/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6730\n",
      "512/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6730\n",
      "513/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6730\n",
      "514/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "515/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6730\n",
      "516/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6730\n",
      "517/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "518/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6730\n",
      "519/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6730\n",
      "520/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6729\n",
      "521/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6729\n",
      "522/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6729\n",
      "523/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6729\n",
      "524/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "525/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6729\n",
      "526/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6729\n",
      "527/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6729\n",
      "528/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6729\n",
      "529/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "530/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6729\n",
      "531/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6729\n",
      "532/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6729\n",
      "533/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6729\n",
      "534/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6729\n",
      "535/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6729\n",
      "536/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6729\n",
      "537/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6729\n",
      "538/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "539/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6729\n",
      "540/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "541/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6729\n",
      "542/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6729\n",
      "543/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6729\n",
      "544/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6729\n",
      "545/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "546/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6729\n",
      "547/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6729\n",
      "548/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6729\n",
      "549/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "550/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "551/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "552/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "553/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6729\n",
      "554/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6729\n",
      "555/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6729\n",
      "556/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6729\n",
      "557/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6729\n",
      "558/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6728\n",
      "559/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6728\n",
      "560/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6728\n",
      "561/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6728\n",
      "562/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6728\n",
      "563/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6728\n",
      "564/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6728\n",
      "565/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6728\n",
      "566/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6728\n",
      "567/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6728\n",
      "568/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6728\n",
      "569/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6728\n",
      "570/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6728\n",
      "571/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6728\n",
      "572/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6728\n",
      "573/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6728\n",
      "574/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6728\n",
      "575/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6728\n",
      "576/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6728\n",
      "577/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6728\n",
      "578/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6728\n",
      "579/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6728\n",
      "580/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6728\n",
      "581/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6728\n",
      "582/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6728\n",
      "583/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6728\n",
      "584/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6728\n",
      "585/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6728\n",
      "586/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6728\n",
      "587/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6728\n",
      "588/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6728\n",
      "589/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6728\n",
      "590/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6728\n",
      "591/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6728\n",
      "592/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6728\n",
      "593/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6728\n",
      "594/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6727\n",
      "595/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6727\n",
      "596/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6727\n",
      "597/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6727\n",
      "598/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6727\n",
      "599/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6727\n",
      "600/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6727\n",
      "601/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6727\n",
      "602/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6727\n",
      "603/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6727\n",
      "604/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6727\n",
      "605/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6727\n",
      "606/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6727\n",
      "607/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6727\n",
      "608/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6727\n",
      "609/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6727\n",
      "610/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6727\n",
      "611/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6727\n",
      "612/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6727\n",
      "613/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6727\n",
      "614/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6727\n",
      "615/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6727\n",
      "616/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6727\n",
      "617/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6727\n",
      "618/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6727\n",
      "619/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6727\n",
      "620/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6727\n",
      "621/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6727\n",
      "622/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6727\n",
      "623/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6726\n",
      "624/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6727\n",
      "625/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6726\n",
      "626/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6726\n",
      "627/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6726\n",
      "628/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6726\n",
      "629/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6726\n",
      "630/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6726\n",
      "631/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6726\n",
      "632/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6726\n",
      "633/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6726\n",
      "634/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6726\n",
      "635/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6726\n",
      "636/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6726\n",
      "637/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6726\n",
      "638/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6726\n",
      "639/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6726\n",
      "640/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6726\n",
      "641/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6726\n",
      "642/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6726\n",
      "643/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6726\n",
      "644/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6726\n",
      "645/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6726\n",
      "646/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6726\n",
      "647/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6726\n",
      "648/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6726\n",
      "649/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6725\n",
      "650/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6725\n",
      "651/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6725\n",
      "652/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6725\n",
      "653/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6725\n",
      "654/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6725\n",
      "655/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6725\n",
      "656/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6725\n",
      "657/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6725\n",
      "658/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6725\n",
      "659/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6725\n",
      "660/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6725\n",
      "661/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6725\n",
      "662/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6725\n",
      "663/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6725\n",
      "664/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6725\n",
      "665/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6725\n",
      "666/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6725\n",
      "667/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6725\n",
      "668/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6725\n",
      "669/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6725\n",
      "670/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6724\n",
      "671/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6724\n",
      "672/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6724\n",
      "673/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6724\n",
      "674/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6724\n",
      "675/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6724\n",
      "676/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6724\n",
      "677/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6724\n",
      "678/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6724\n",
      "679/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6724\n",
      "680/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6724\n",
      "681/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6724\n",
      "682/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6724\n",
      "683/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6724\n",
      "684/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6724\n",
      "685/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6724\n",
      "686/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6724\n",
      "687/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6723\n",
      "688/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6723\n",
      "689/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6723\n",
      "690/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6723\n",
      "691/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6723\n",
      "692/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6723\n",
      "693/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6723\n",
      "694/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6723\n",
      "695/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6723\n",
      "696/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6723\n",
      "697/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6723\n",
      "698/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6723\n",
      "699/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6723\n",
      "700/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6723\n",
      "701/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6723\n",
      "702/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6723\n",
      "703/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6723\n",
      "704/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6722\n",
      "705/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6722\n",
      "706/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6722\n",
      "707/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6722\n",
      "708/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6722\n",
      "709/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6722\n",
      "710/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6722\n",
      "711/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6722\n",
      "712/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6722\n",
      "713/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6722\n",
      "714/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6722\n",
      "715/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6722\n",
      "716/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6722\n",
      "717/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6722\n",
      "718/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6722\n",
      "719/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6721\n",
      "720/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6721\n",
      "721/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6721\n",
      "722/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6721\n",
      "723/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6721\n",
      "724/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6721\n",
      "725/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6721\n",
      "726/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6721\n",
      "727/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6721\n",
      "728/1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6631 Validation Loss: 0.6721\n",
      "729/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6721\n",
      "730/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6721\n",
      "731/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6721\n",
      "732/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6721\n",
      "733/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6720\n",
      "734/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6720\n",
      "735/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6720\n",
      "736/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6720\n",
      "737/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6720\n",
      "738/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6720\n",
      "739/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6720\n",
      "740/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6720\n",
      "741/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6720\n",
      "742/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6720\n",
      "743/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6720\n",
      "744/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6720\n",
      "745/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6720\n",
      "746/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6719\n",
      "747/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6719\n",
      "748/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6719\n",
      "749/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6719\n",
      "750/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6719\n",
      "751/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6719\n",
      "752/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6719\n",
      "753/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6719\n",
      "754/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6719\n",
      "755/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6719\n",
      "756/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6719\n",
      "757/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6719\n",
      "758/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6718\n",
      "759/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6718\n",
      "760/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6718\n",
      "761/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6718\n",
      "762/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6718\n",
      "763/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6718\n",
      "764/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6718\n",
      "765/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6718\n",
      "766/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6718\n",
      "767/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6718\n",
      "768/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6717\n",
      "769/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6717\n",
      "770/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6717\n",
      "771/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6717\n",
      "772/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6717\n",
      "773/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6717\n",
      "774/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6717\n",
      "775/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6717\n",
      "776/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6717\n",
      "777/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6717\n",
      "778/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6716\n",
      "779/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6716\n",
      "780/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6716\n",
      "781/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6716\n",
      "782/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6716\n",
      "783/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6716\n",
      "784/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6716\n",
      "785/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6716\n",
      "786/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6715\n",
      "787/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6715\n",
      "788/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6715\n",
      "789/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6715\n",
      "790/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6715\n",
      "791/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6715\n",
      "792/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6715\n",
      "793/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6715\n",
      "794/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6715\n",
      "795/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6714\n",
      "796/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6714\n",
      "797/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6714\n",
      "798/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6714\n",
      "799/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6714\n",
      "800/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6714\n",
      "801/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6714\n",
      "802/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6713\n",
      "803/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6713\n",
      "804/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6713\n",
      "805/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6713\n",
      "806/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6713\n",
      "807/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6713\n",
      "808/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6713\n",
      "809/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6713\n",
      "810/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6712\n",
      "811/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6712\n",
      "812/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6712\n",
      "813/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6712\n",
      "814/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6712\n",
      "815/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6712\n",
      "816/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6712\n",
      "817/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6711\n",
      "818/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6711\n",
      "819/1000:\n",
      "Training Loss: 0.6609 Validation Loss: 0.6711\n",
      "820/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6711\n",
      "821/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6711\n",
      "822/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6711\n",
      "823/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6711\n",
      "824/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6710\n",
      "825/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6710\n",
      "826/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6710\n",
      "827/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6710\n",
      "828/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6710\n",
      "829/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6710\n",
      "830/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6709\n",
      "831/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6709\n",
      "832/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6709\n",
      "833/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6709\n",
      "834/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6708\n",
      "835/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6708\n",
      "836/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6708\n",
      "837/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6708\n",
      "838/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6708\n",
      "839/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6707\n",
      "840/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6707\n",
      "841/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6707\n",
      "842/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6707\n",
      "843/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6706\n",
      "844/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6706\n",
      "845/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6706\n",
      "846/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6705\n",
      "847/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6705\n",
      "848/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6705\n",
      "849/1000:\n",
      "Training Loss: 0.6593 Validation Loss: 0.6705\n",
      "850/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6704\n",
      "851/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6704\n",
      "852/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6704\n",
      "853/1000:\n",
      "Training Loss: 0.6609 Validation Loss: 0.6704\n",
      "854/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6703\n",
      "855/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6703\n",
      "856/1000:\n",
      "Training Loss: 0.6609 Validation Loss: 0.6703\n",
      "857/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6702\n",
      "858/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6702\n",
      "859/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6702\n",
      "860/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6701\n",
      "861/1000:\n",
      "Training Loss: 0.6586 Validation Loss: 0.6701\n",
      "862/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6701\n",
      "863/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6700\n",
      "864/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6700\n",
      "865/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6700\n",
      "866/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6699\n",
      "867/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6699\n",
      "868/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6699\n",
      "869/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6698\n",
      "870/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6698\n",
      "871/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6698\n",
      "872/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6697\n",
      "873/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6697\n",
      "874/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6697\n",
      "875/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6696\n",
      "876/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6696\n",
      "877/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6696\n",
      "878/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6695\n",
      "879/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6695\n",
      "880/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6694\n",
      "881/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6694\n",
      "882/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6694\n",
      "883/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6693\n",
      "884/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6693\n",
      "885/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6692\n",
      "886/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6692\n",
      "887/1000:\n",
      "Training Loss: 0.6597 Validation Loss: 0.6692\n",
      "888/1000:\n",
      "Training Loss: 0.6597 Validation Loss: 0.6691\n",
      "889/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6691\n",
      "890/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6691\n",
      "891/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6690\n",
      "892/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6690\n",
      "893/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6689\n",
      "894/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6689\n",
      "895/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6689\n",
      "896/1000:\n",
      "Training Loss: 0.6595 Validation Loss: 0.6688\n",
      "897/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6688\n",
      "898/1000:\n",
      "Training Loss: 0.6598 Validation Loss: 0.6687\n",
      "899/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6687\n",
      "900/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6686\n",
      "901/1000:\n",
      "Training Loss: 0.6588 Validation Loss: 0.6686\n",
      "902/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6685\n",
      "903/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6685\n",
      "904/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6684\n",
      "905/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6684\n",
      "906/1000:\n",
      "Training Loss: 0.6596 Validation Loss: 0.6683\n",
      "907/1000:\n",
      "Training Loss: 0.6591 Validation Loss: 0.6682\n",
      "908/1000:\n",
      "Training Loss: 0.6585 Validation Loss: 0.6682\n",
      "909/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6682\n",
      "910/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6681\n",
      "911/1000:\n",
      "Training Loss: 0.6589 Validation Loss: 0.6681\n",
      "912/1000:\n",
      "Training Loss: 0.6597 Validation Loss: 0.6680\n",
      "913/1000:\n",
      "Training Loss: 0.6592 Validation Loss: 0.6680\n",
      "914/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6679\n",
      "915/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6679\n",
      "916/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6678\n",
      "917/1000:\n",
      "Training Loss: 0.6593 Validation Loss: 0.6678\n",
      "918/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6677\n",
      "919/1000:\n",
      "Training Loss: 0.6578 Validation Loss: 0.6676\n",
      "920/1000:\n",
      "Training Loss: 0.6577 Validation Loss: 0.6676\n",
      "921/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6675\n",
      "922/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6675\n",
      "923/1000:\n",
      "Training Loss: 0.6586 Validation Loss: 0.6674\n",
      "924/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6674\n",
      "925/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6673\n",
      "926/1000:\n",
      "Training Loss: 0.6598 Validation Loss: 0.6672\n",
      "927/1000:\n",
      "Training Loss: 0.6592 Validation Loss: 0.6672\n",
      "928/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6671\n",
      "929/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6670\n",
      "930/1000:\n",
      "Training Loss: 0.6586 Validation Loss: 0.6670\n",
      "931/1000:\n",
      "Training Loss: 0.6574 Validation Loss: 0.6669\n",
      "932/1000:\n",
      "Training Loss: 0.6576 Validation Loss: 0.6668\n",
      "933/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6667\n",
      "934/1000:\n",
      "Training Loss: 0.6583 Validation Loss: 0.6667\n",
      "935/1000:\n",
      "Training Loss: 0.6580 Validation Loss: 0.6666\n",
      "936/1000:\n",
      "Training Loss: 0.6593 Validation Loss: 0.6665\n",
      "937/1000:\n",
      "Training Loss: 0.6588 Validation Loss: 0.6665\n",
      "938/1000:\n",
      "Training Loss: 0.6575 Validation Loss: 0.6664\n",
      "939/1000:\n",
      "Training Loss: 0.6589 Validation Loss: 0.6663\n",
      "940/1000:\n",
      "Training Loss: 0.6583 Validation Loss: 0.6662\n",
      "941/1000:\n",
      "Training Loss: 0.6582 Validation Loss: 0.6662\n",
      "942/1000:\n",
      "Training Loss: 0.6573 Validation Loss: 0.6661\n",
      "943/1000:\n",
      "Training Loss: 0.6582 Validation Loss: 0.6660\n",
      "944/1000:\n",
      "Training Loss: 0.6590 Validation Loss: 0.6659\n",
      "945/1000:\n",
      "Training Loss: 0.6588 Validation Loss: 0.6659\n",
      "946/1000:\n",
      "Training Loss: 0.6575 Validation Loss: 0.6658\n",
      "947/1000:\n",
      "Training Loss: 0.6581 Validation Loss: 0.6657\n",
      "948/1000:\n",
      "Training Loss: 0.6575 Validation Loss: 0.6656\n",
      "949/1000:\n",
      "Training Loss: 0.6580 Validation Loss: 0.6655\n",
      "950/1000:\n",
      "Training Loss: 0.6568 Validation Loss: 0.6655\n",
      "951/1000:\n",
      "Training Loss: 0.6560 Validation Loss: 0.6654\n",
      "952/1000:\n",
      "Training Loss: 0.6572 Validation Loss: 0.6653\n",
      "953/1000:\n",
      "Training Loss: 0.6580 Validation Loss: 0.6652\n",
      "954/1000:\n",
      "Training Loss: 0.6583 Validation Loss: 0.6651\n",
      "955/1000:\n",
      "Training Loss: 0.6549 Validation Loss: 0.6650\n",
      "956/1000:\n",
      "Training Loss: 0.6574 Validation Loss: 0.6649\n",
      "957/1000:\n",
      "Training Loss: 0.6569 Validation Loss: 0.6648\n",
      "958/1000:\n",
      "Training Loss: 0.6580 Validation Loss: 0.6647\n",
      "959/1000:\n",
      "Training Loss: 0.6556 Validation Loss: 0.6646\n",
      "960/1000:\n",
      "Training Loss: 0.6568 Validation Loss: 0.6645\n",
      "961/1000:\n",
      "Training Loss: 0.6561 Validation Loss: 0.6644\n",
      "962/1000:\n",
      "Training Loss: 0.6570 Validation Loss: 0.6643\n",
      "963/1000:\n",
      "Training Loss: 0.6567 Validation Loss: 0.6642\n",
      "964/1000:\n",
      "Training Loss: 0.6573 Validation Loss: 0.6641\n",
      "965/1000:\n",
      "Training Loss: 0.6553 Validation Loss: 0.6640\n",
      "966/1000:\n",
      "Training Loss: 0.6560 Validation Loss: 0.6639\n",
      "967/1000:\n",
      "Training Loss: 0.6569 Validation Loss: 0.6637\n",
      "968/1000:\n",
      "Training Loss: 0.6559 Validation Loss: 0.6636\n",
      "969/1000:\n",
      "Training Loss: 0.6536 Validation Loss: 0.6635\n",
      "970/1000:\n",
      "Training Loss: 0.6547 Validation Loss: 0.6634\n",
      "971/1000:\n",
      "Training Loss: 0.6555 Validation Loss: 0.6632\n",
      "972/1000:\n",
      "Training Loss: 0.6564 Validation Loss: 0.6631\n",
      "973/1000:\n",
      "Training Loss: 0.6568 Validation Loss: 0.6630\n",
      "974/1000:\n",
      "Training Loss: 0.6554 Validation Loss: 0.6629\n",
      "975/1000:\n",
      "Training Loss: 0.6563 Validation Loss: 0.6628\n",
      "976/1000:\n",
      "Training Loss: 0.6558 Validation Loss: 0.6626\n",
      "977/1000:\n",
      "Training Loss: 0.6536 Validation Loss: 0.6625\n",
      "978/1000:\n",
      "Training Loss: 0.6544 Validation Loss: 0.6623\n",
      "979/1000:\n",
      "Training Loss: 0.6535 Validation Loss: 0.6622\n",
      "980/1000:\n",
      "Training Loss: 0.6558 Validation Loss: 0.6620\n",
      "981/1000:\n",
      "Training Loss: 0.6540 Validation Loss: 0.6619\n",
      "982/1000:\n",
      "Training Loss: 0.6548 Validation Loss: 0.6617\n",
      "983/1000:\n",
      "Training Loss: 0.6564 Validation Loss: 0.6616\n",
      "984/1000:\n",
      "Training Loss: 0.6552 Validation Loss: 0.6615\n",
      "985/1000:\n",
      "Training Loss: 0.6563 Validation Loss: 0.6613\n",
      "986/1000:\n",
      "Training Loss: 0.6565 Validation Loss: 0.6611\n",
      "987/1000:\n",
      "Training Loss: 0.6548 Validation Loss: 0.6610\n",
      "988/1000:\n",
      "Training Loss: 0.6537 Validation Loss: 0.6608\n",
      "989/1000:\n",
      "Training Loss: 0.6543 Validation Loss: 0.6607\n",
      "990/1000:\n",
      "Training Loss: 0.6541 Validation Loss: 0.6605\n",
      "991/1000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6542 Validation Loss: 0.6604\n",
      "992/1000:\n",
      "Training Loss: 0.6544 Validation Loss: 0.6602\n",
      "993/1000:\n",
      "Training Loss: 0.6525 Validation Loss: 0.6600\n",
      "994/1000:\n",
      "Training Loss: 0.6524 Validation Loss: 0.6599\n",
      "995/1000:\n",
      "Training Loss: 0.6518 Validation Loss: 0.6597\n",
      "996/1000:\n",
      "Training Loss: 0.6520 Validation Loss: 0.6595\n",
      "997/1000:\n",
      "Training Loss: 0.6543 Validation Loss: 0.6593\n",
      "998/1000:\n",
      "Training Loss: 0.6527 Validation Loss: 0.6592\n",
      "999/1000:\n",
      "Training Loss: 0.6525 Validation Loss: 0.6590\n",
      "1000/1000:\n",
      "Training Loss: 0.6551 Validation Loss: 0.6588\n"
     ]
    }
   ],
   "source": [
    "second_model = second_model.fit(X_train, y_train, X_test, y_test, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ac2e576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.600896860986547"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = second_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96a6b878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03be9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(second_model, 'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3ebb481",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module.load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msecond_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "second_model.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c0b443e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMLPModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.2, inplace=False)\n",
       "    (15): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (16): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
