{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b6fa10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:39:58.053092800Z",
     "start_time": "2023-08-01T13:39:48.482921700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3c0cb",
   "metadata": {},
   "source": [
    "### First, we load the titatic dataset and convert all categorical variables to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6d93d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:39:58.133275Z",
     "start_time": "2023-08-01T13:39:58.054073600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  Survived        Age  SibSp  Parch     Fare  Pclass_1   \n0              1         0  22.000000      1      0   7.2500     False  \\\n1              2         1  38.000000      1      0  71.2833      True   \n2              3         1  26.000000      0      0   7.9250     False   \n3              4         1  35.000000      1      0  53.1000      True   \n4              5         0  35.000000      0      0   8.0500     False   \n..           ...       ...        ...    ...    ...      ...       ...   \n886          887         0  27.000000      0      0  13.0000     False   \n887          888         1  19.000000      0      0  30.0000      True   \n888          889         0  29.699118      1      2  23.4500     False   \n889          890         1  26.000000      0      0  30.0000      True   \n890          891         0  32.000000      0      0   7.7500     False   \n\n     Pclass_2  Pclass_3  Sex_female  Sex_male  Embarked_C  Embarked_Q   \n0       False      True       False      True       False       False  \\\n1       False     False        True     False        True       False   \n2       False      True        True     False       False       False   \n3       False     False        True     False       False       False   \n4       False      True       False      True       False       False   \n..        ...       ...         ...       ...         ...         ...   \n886      True     False       False      True       False       False   \n887     False     False        True     False       False       False   \n888     False      True        True     False       False       False   \n889     False     False       False      True        True       False   \n890     False      True       False      True       False        True   \n\n     Embarked_S  \n0          True  \n1         False  \n2          True  \n3          True  \n4          True  \n..          ...  \n886        True  \n887        True  \n888        True  \n889       False  \n890       False  \n\n[891 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>22.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>38.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>26.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>35.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>35.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>0</td>\n      <td>27.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>19.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>0</td>\n      <td>29.699118</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>26.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>0</td>\n      <td>32.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv('titanic.csv')\n",
    "data_frame = data_frame.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "data_frame = pd.get_dummies(data_frame, columns=['Pclass', 'Sex', 'Embarked'])\n",
    "data_frame = data_frame.fillna(data_frame.mean())\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d66e7",
   "metadata": {},
   "source": [
    "### Next, we convert the dataset to a NumPy array and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9dcacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:39:58.133275Z",
     "start_time": "2023-08-01T13:39:58.114068800Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = data_frame[['Survived']].to_numpy().astype(float)\n",
    "observ = data_frame.drop(['PassengerId', 'Survived'], axis=1).to_numpy().astype(float)\n",
    "X_train, X_test, y_train, y_test = train_test_split(observ, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77af664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:39:58.197043Z",
     "start_time": "2023-08-01T13:39:58.132295900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(891, 12)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea47c718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:39:58.198034100Z",
     "start_time": "2023-08-01T13:39:58.152883900Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have to normalize Age and Fare, so columns 0,3\n",
    "age_min, age_max = X_train[:,0].min(), X_train[:,0].max()\n",
    "fare_min, fare_max = X_train[:,3].min(), X_train[:,3].max()\n",
    "# Apply\n",
    "X_train[:,0] = (X_train[:,0] - age_min) / (age_max - age_min + 1e-5)\n",
    "X_test[:,0] = (X_test[:,0] - age_min) / (age_max - age_min + 1e-5)\n",
    "X_train[:,3] = (X_train[:,3] - fare_min) / (fare_max - fare_min + 1e-5)\n",
    "X_test[:,3] = (X_test[:,3] - fare_min) / (fare_max - fare_min + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05a1c9",
   "metadata": {},
   "source": [
    "### Now, let's create our sequential model with four linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ccd53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:06.537349800Z",
     "start_time": "2023-08-01T13:39:58.164096200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "first_model = [\n",
    "    nn.Linear(X_train.shape[1], 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    \n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8526e",
   "metadata": {},
   "source": [
    "### We can already apply it to our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d54baa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:28.740020700Z",
     "start_time": "2023-08-01T13:40:28.708634900Z"
    }
   },
   "outputs": [],
   "source": [
    "inp = torch.tensor(X_test[:20], dtype=torch.float)\n",
    "for layer in first_model:\n",
    "    inp = layer(inp)\n",
    "#first_model(torch.tensor(X_test[:20], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0cef270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:29.424094Z",
     "start_time": "2023-08-01T13:40:29.363702100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4901],\n        [0.4978],\n        [0.4926],\n        [0.4977],\n        [0.4836],\n        [0.5025],\n        [0.5027],\n        [0.5051],\n        [0.5166],\n        [0.5198],\n        [0.4911],\n        [0.4705],\n        [0.5135],\n        [0.5058],\n        [0.5016],\n        [0.4951],\n        [0.4732],\n        [0.4869],\n        [0.4860],\n        [0.4828]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67138f74",
   "metadata": {},
   "source": [
    "### Now, let's fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bd8f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:31.082620900Z",
     "start_time": "2023-08-01T13:40:31.026591600Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfirst_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m(X_train, y_train, X_test, y_test)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "first_model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8973e8",
   "metadata": {},
   "source": [
    "### As you may have seen, this does not work. We have to implement the training ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b345d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:36.163713Z",
     "start_time": "2023-08-01T13:40:36.124402Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMLPModel(nn.Module):\n",
    "    def __init__(self, input, *hidden_layers, lr=0.1, dropout=0.2):\n",
    "        super().__init__() # <- Very important!\n",
    "        self.lr = lr\n",
    "        ## Build model\n",
    "        n_neurons = [input] + list(hidden_layers)\n",
    "        self.layers = []\n",
    "        for i, o in zip(n_neurons[:-1], n_neurons[1:]):\n",
    "            self.layers += [\n",
    "                nn.Linear(i, o),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ]\n",
    "        self.layers += [\n",
    "            nn.Linear(hidden_layers[-1], 1), # Output layer with no final activation\n",
    "            nn.Sigmoid(), # Final activation function\n",
    "        ]\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers) # Create a sequential model\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    def predict(self, X, th=0.5):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X)\n",
    "        return (y_hat >= th).float()\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        y_hat = self(X)\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "        \n",
    "    def validation_step(self, X, y):\n",
    "        with torch.no_grad():\n",
    "            return self.train_step(X, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid, epochs=10):\n",
    "        ## Convert dataset\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "        \n",
    "        X_valid = torch.tensor(X_valid, dtype=torch.float)\n",
    "        y_valid = torch.tensor(y_valid, dtype=torch.float)\n",
    "        \n",
    "        ## Load Optimizer\n",
    "        optimizer = self.configure_optimizers()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'{epoch+1}/{epochs}:')\n",
    "            # Training\n",
    "            self.train() # Set model to training mode\n",
    "            optimizer.zero_grad(set_to_none=True) # Sets all gradients to Zero (Default is to None) \n",
    "            loss = self.train_step(X_train, y_train) # Execute Forward pass and calculate Loss\n",
    "            loss.backward() # Execute Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "            self.eval() # Set model to validation mode\n",
    "            \n",
    "            # Validation\n",
    "            loss_valid = self.validation_step(X_valid, y_valid)\n",
    "            print(f'Training Loss: {loss.item():1.4f}', f'Validation Loss: {loss_valid.item():1.4f}')\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad180e6",
   "metadata": {},
   "source": [
    "### Create a model similar to the one before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8683b5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:37.216054900Z",
     "start_time": "2023-08-01T13:40:37.127488700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MyMLPModel(\n  (layers): Sequential(\n    (0): Linear(in_features=12, out_features=32, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=32, out_features=16, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=16, out_features=8, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.2, inplace=False)\n    (9): Linear(in_features=8, out_features=4, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2, inplace=False)\n    (12): Linear(in_features=4, out_features=1, bias=True)\n    (13): Sigmoid()\n  )\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model = MyMLPModel(X_train.shape[1], 32, 16, 8, 4)\n",
    "second_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9ec39e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:38.047066100Z",
     "start_time": "2023-08-01T13:40:37.984317400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.predict(X_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a151d8",
   "metadata": {},
   "source": [
    "### Train it and calculate the test accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae2b864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:42.541504100Z",
     "start_time": "2023-08-01T13:40:39.491942100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000:\n",
      "Training Loss: 0.7043 Validation Loss: 0.7012\n",
      "2/1000:\n",
      "Training Loss: 0.7017 Validation Loss: 0.6991\n",
      "3/1000:\n",
      "Training Loss: 0.6997 Validation Loss: 0.6973\n",
      "4/1000:\n",
      "Training Loss: 0.6977 Validation Loss: 0.6955\n",
      "5/1000:\n",
      "Training Loss: 0.6953 Validation Loss: 0.6939\n",
      "6/1000:\n",
      "Training Loss: 0.6945 Validation Loss: 0.6923\n",
      "7/1000:\n",
      "Training Loss: 0.6911 Validation Loss: 0.6908\n",
      "8/1000:\n",
      "Training Loss: 0.6894 Validation Loss: 0.6894\n",
      "9/1000:\n",
      "Training Loss: 0.6886 Validation Loss: 0.6882\n",
      "10/1000:\n",
      "Training Loss: 0.6865 Validation Loss: 0.6870\n",
      "11/1000:\n",
      "Training Loss: 0.6853 Validation Loss: 0.6858\n",
      "12/1000:\n",
      "Training Loss: 0.6842 Validation Loss: 0.6848\n",
      "13/1000:\n",
      "Training Loss: 0.6818 Validation Loss: 0.6838\n",
      "14/1000:\n",
      "Training Loss: 0.6816 Validation Loss: 0.6829\n",
      "15/1000:\n",
      "Training Loss: 0.6808 Validation Loss: 0.6821\n",
      "16/1000:\n",
      "Training Loss: 0.6795 Validation Loss: 0.6813\n",
      "17/1000:\n",
      "Training Loss: 0.6783 Validation Loss: 0.6806\n",
      "18/1000:\n",
      "Training Loss: 0.6778 Validation Loss: 0.6799\n",
      "19/1000:\n",
      "Training Loss: 0.6773 Validation Loss: 0.6793\n",
      "20/1000:\n",
      "Training Loss: 0.6755 Validation Loss: 0.6787\n",
      "21/1000:\n",
      "Training Loss: 0.6750 Validation Loss: 0.6782\n",
      "22/1000:\n",
      "Training Loss: 0.6742 Validation Loss: 0.6777\n",
      "23/1000:\n",
      "Training Loss: 0.6722 Validation Loss: 0.6773\n",
      "24/1000:\n",
      "Training Loss: 0.6749 Validation Loss: 0.6769\n",
      "25/1000:\n",
      "Training Loss: 0.6719 Validation Loss: 0.6765\n",
      "26/1000:\n",
      "Training Loss: 0.6728 Validation Loss: 0.6762\n",
      "27/1000:\n",
      "Training Loss: 0.6712 Validation Loss: 0.6758\n",
      "28/1000:\n",
      "Training Loss: 0.6687 Validation Loss: 0.6755\n",
      "29/1000:\n",
      "Training Loss: 0.6705 Validation Loss: 0.6752\n",
      "30/1000:\n",
      "Training Loss: 0.6696 Validation Loss: 0.6750\n",
      "31/1000:\n",
      "Training Loss: 0.6689 Validation Loss: 0.6748\n",
      "32/1000:\n",
      "Training Loss: 0.6695 Validation Loss: 0.6745\n",
      "33/1000:\n",
      "Training Loss: 0.6690 Validation Loss: 0.6744\n",
      "34/1000:\n",
      "Training Loss: 0.6707 Validation Loss: 0.6742\n",
      "35/1000:\n",
      "Training Loss: 0.6698 Validation Loss: 0.6740\n",
      "36/1000:\n",
      "Training Loss: 0.6667 Validation Loss: 0.6739\n",
      "37/1000:\n",
      "Training Loss: 0.6678 Validation Loss: 0.6738\n",
      "38/1000:\n",
      "Training Loss: 0.6672 Validation Loss: 0.6736\n",
      "39/1000:\n",
      "Training Loss: 0.6674 Validation Loss: 0.6735\n",
      "40/1000:\n",
      "Training Loss: 0.6672 Validation Loss: 0.6734\n",
      "41/1000:\n",
      "Training Loss: 0.6651 Validation Loss: 0.6733\n",
      "42/1000:\n",
      "Training Loss: 0.6685 Validation Loss: 0.6732\n",
      "43/1000:\n",
      "Training Loss: 0.6679 Validation Loss: 0.6732\n",
      "44/1000:\n",
      "Training Loss: 0.6667 Validation Loss: 0.6731\n",
      "45/1000:\n",
      "Training Loss: 0.6659 Validation Loss: 0.6731\n",
      "46/1000:\n",
      "Training Loss: 0.6686 Validation Loss: 0.6730\n",
      "47/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6730\n",
      "48/1000:\n",
      "Training Loss: 0.6658 Validation Loss: 0.6729\n",
      "49/1000:\n",
      "Training Loss: 0.6666 Validation Loss: 0.6729\n",
      "50/1000:\n",
      "Training Loss: 0.6668 Validation Loss: 0.6729\n",
      "51/1000:\n",
      "Training Loss: 0.6649 Validation Loss: 0.6728\n",
      "52/1000:\n",
      "Training Loss: 0.6670 Validation Loss: 0.6728\n",
      "53/1000:\n",
      "Training Loss: 0.6662 Validation Loss: 0.6728\n",
      "54/1000:\n",
      "Training Loss: 0.6666 Validation Loss: 0.6728\n",
      "55/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6728\n",
      "56/1000:\n",
      "Training Loss: 0.6651 Validation Loss: 0.6728\n",
      "57/1000:\n",
      "Training Loss: 0.6662 Validation Loss: 0.6727\n",
      "58/1000:\n",
      "Training Loss: 0.6656 Validation Loss: 0.6727\n",
      "59/1000:\n",
      "Training Loss: 0.6659 Validation Loss: 0.6727\n",
      "60/1000:\n",
      "Training Loss: 0.6666 Validation Loss: 0.6727\n",
      "61/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6727\n",
      "62/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6727\n",
      "63/1000:\n",
      "Training Loss: 0.6667 Validation Loss: 0.6727\n",
      "64/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6727\n",
      "65/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6727\n",
      "66/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6727\n",
      "67/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6727\n",
      "68/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6727\n",
      "69/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6727\n",
      "70/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6727\n",
      "71/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6728\n",
      "72/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6728\n",
      "73/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6728\n",
      "74/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6728\n",
      "75/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6728\n",
      "76/1000:\n",
      "Training Loss: 0.6654 Validation Loss: 0.6728\n",
      "77/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6728\n",
      "78/1000:\n",
      "Training Loss: 0.6660 Validation Loss: 0.6728\n",
      "79/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6728\n",
      "80/1000:\n",
      "Training Loss: 0.6646 Validation Loss: 0.6728\n",
      "81/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6728\n",
      "82/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6728\n",
      "83/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6728\n",
      "84/1000:\n",
      "Training Loss: 0.6662 Validation Loss: 0.6728\n",
      "85/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6728\n",
      "86/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6729\n",
      "87/1000:\n",
      "Training Loss: 0.6656 Validation Loss: 0.6729\n",
      "88/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6729\n",
      "89/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6729\n",
      "90/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6729\n",
      "91/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6729\n",
      "92/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "93/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6729\n",
      "94/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6729\n",
      "95/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6729\n",
      "96/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6729\n",
      "97/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6729\n",
      "98/1000:\n",
      "Training Loss: 0.6654 Validation Loss: 0.6729\n",
      "99/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6729\n",
      "100/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6729\n",
      "101/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6729\n",
      "102/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6729\n",
      "103/1000:\n",
      "Training Loss: 0.6639 Validation Loss: 0.6729\n",
      "104/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6729\n",
      "105/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6729\n",
      "106/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6729\n",
      "107/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6729\n",
      "108/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6729\n",
      "109/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6729\n",
      "110/1000:\n",
      "Training Loss: 0.6637 Validation Loss: 0.6729\n",
      "111/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "112/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6729\n",
      "113/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6729\n",
      "114/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6729\n",
      "115/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6729\n",
      "116/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6729\n",
      "117/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6729\n",
      "118/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6729\n",
      "119/1000:\n",
      "Training Loss: 0.6652 Validation Loss: 0.6729\n",
      "120/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6729\n",
      "121/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6729\n",
      "122/1000:\n",
      "Training Loss: 0.6657 Validation Loss: 0.6728\n",
      "123/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6728\n",
      "124/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6728\n",
      "125/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6728\n",
      "126/1000:\n",
      "Training Loss: 0.6646 Validation Loss: 0.6728\n",
      "127/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6728\n",
      "128/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6728\n",
      "129/1000:\n",
      "Training Loss: 0.6658 Validation Loss: 0.6728\n",
      "130/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6728\n",
      "131/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6728\n",
      "132/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6728\n",
      "133/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6728\n",
      "134/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6728\n",
      "135/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6728\n",
      "136/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6728\n",
      "137/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6728\n",
      "138/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6728\n",
      "139/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6728\n",
      "140/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6728\n",
      "141/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6728\n",
      "142/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6728\n",
      "143/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6728\n",
      "144/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6728\n",
      "145/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6727\n",
      "146/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6727\n",
      "147/1000:\n",
      "Training Loss: 0.6646 Validation Loss: 0.6727\n",
      "148/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6727\n",
      "149/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6727\n",
      "150/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6727\n",
      "151/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6727\n",
      "152/1000:\n",
      "Training Loss: 0.6646 Validation Loss: 0.6727\n",
      "153/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6727\n",
      "154/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6727\n",
      "155/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6726\n",
      "156/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6726\n",
      "157/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6726\n",
      "158/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6726\n",
      "159/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6726\n",
      "160/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6726\n",
      "161/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6726\n",
      "162/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6726\n",
      "163/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6726\n",
      "164/1000:\n",
      "Training Loss: 0.6629 Validation Loss: 0.6726\n",
      "165/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6726\n",
      "166/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6725\n",
      "167/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6725\n",
      "168/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6725\n",
      "169/1000:\n",
      "Training Loss: 0.6649 Validation Loss: 0.6725\n",
      "170/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6725\n",
      "171/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6725\n",
      "172/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6725\n",
      "173/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6725\n",
      "174/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6724\n",
      "175/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6724\n",
      "176/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6724\n",
      "177/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6724\n",
      "178/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6724\n",
      "179/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6724\n",
      "180/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6724\n",
      "181/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6724\n",
      "182/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6724\n",
      "183/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6724\n",
      "184/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6724\n",
      "185/1000:\n",
      "Training Loss: 0.6640 Validation Loss: 0.6723\n",
      "186/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6723\n",
      "187/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6723\n",
      "188/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6723\n",
      "189/1000:\n",
      "Training Loss: 0.6643 Validation Loss: 0.6723\n",
      "190/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6723\n",
      "191/1000:\n",
      "Training Loss: 0.6645 Validation Loss: 0.6723\n",
      "192/1000:\n",
      "Training Loss: 0.6644 Validation Loss: 0.6722\n",
      "193/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6722\n",
      "194/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6722\n",
      "195/1000:\n",
      "Training Loss: 0.6634 Validation Loss: 0.6722\n",
      "196/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6722\n",
      "197/1000:\n",
      "Training Loss: 0.6607 Validation Loss: 0.6722\n",
      "198/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6722\n",
      "199/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6722\n",
      "200/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6721\n",
      "201/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6721\n",
      "202/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6721\n",
      "203/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6721\n",
      "204/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6721\n",
      "205/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6721\n",
      "206/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6720\n",
      "207/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6720\n",
      "208/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6720\n",
      "209/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6720\n",
      "210/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6720\n",
      "211/1000:\n",
      "Training Loss: 0.6650 Validation Loss: 0.6719\n",
      "212/1000:\n",
      "Training Loss: 0.6615 Validation Loss: 0.6719\n",
      "213/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6719\n",
      "214/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6719\n",
      "215/1000:\n",
      "Training Loss: 0.6636 Validation Loss: 0.6719\n",
      "216/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6719\n",
      "217/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6718\n",
      "218/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6718\n",
      "219/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6718\n",
      "220/1000:\n",
      "Training Loss: 0.6638 Validation Loss: 0.6718\n",
      "221/1000:\n",
      "Training Loss: 0.6648 Validation Loss: 0.6718\n",
      "222/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6717\n",
      "223/1000:\n",
      "Training Loss: 0.6588 Validation Loss: 0.6717\n",
      "224/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6717\n",
      "225/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6717\n",
      "226/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6717\n",
      "227/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6717\n",
      "228/1000:\n",
      "Training Loss: 0.6598 Validation Loss: 0.6716\n",
      "229/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6716\n",
      "230/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6716\n",
      "231/1000:\n",
      "Training Loss: 0.6630 Validation Loss: 0.6716\n",
      "232/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6715\n",
      "233/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6715\n",
      "234/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6715\n",
      "235/1000:\n",
      "Training Loss: 0.6628 Validation Loss: 0.6715\n",
      "236/1000:\n",
      "Training Loss: 0.6626 Validation Loss: 0.6715\n",
      "237/1000:\n",
      "Training Loss: 0.6618 Validation Loss: 0.6714\n",
      "238/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6714\n",
      "239/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6714\n",
      "240/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6714\n",
      "241/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6713\n",
      "242/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6713\n",
      "243/1000:\n",
      "Training Loss: 0.6632 Validation Loss: 0.6713\n",
      "244/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6713\n",
      "245/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6713\n",
      "246/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6712\n",
      "247/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6712\n",
      "248/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6712\n",
      "249/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6712\n",
      "250/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6711\n",
      "251/1000:\n",
      "Training Loss: 0.6619 Validation Loss: 0.6711\n",
      "252/1000:\n",
      "Training Loss: 0.6625 Validation Loss: 0.6711\n",
      "253/1000:\n",
      "Training Loss: 0.6641 Validation Loss: 0.6710\n",
      "254/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6710\n",
      "255/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6710\n",
      "256/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6710\n",
      "257/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6709\n",
      "258/1000:\n",
      "Training Loss: 0.6610 Validation Loss: 0.6709\n",
      "259/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6709\n",
      "260/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6708\n",
      "261/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6708\n",
      "262/1000:\n",
      "Training Loss: 0.6642 Validation Loss: 0.6708\n",
      "263/1000:\n",
      "Training Loss: 0.6623 Validation Loss: 0.6708\n",
      "264/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6707\n",
      "265/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6707\n",
      "266/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6707\n",
      "267/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6706\n",
      "268/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6706\n",
      "269/1000:\n",
      "Training Loss: 0.6601 Validation Loss: 0.6706\n",
      "270/1000:\n",
      "Training Loss: 0.6595 Validation Loss: 0.6705\n",
      "271/1000:\n",
      "Training Loss: 0.6633 Validation Loss: 0.6705\n",
      "272/1000:\n",
      "Training Loss: 0.6601 Validation Loss: 0.6705\n",
      "273/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6704\n",
      "274/1000:\n",
      "Training Loss: 0.6621 Validation Loss: 0.6704\n",
      "275/1000:\n",
      "Training Loss: 0.6635 Validation Loss: 0.6704\n",
      "276/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6703\n",
      "277/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6703\n",
      "278/1000:\n",
      "Training Loss: 0.6588 Validation Loss: 0.6703\n",
      "279/1000:\n",
      "Training Loss: 0.6612 Validation Loss: 0.6702\n",
      "280/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6702\n",
      "281/1000:\n",
      "Training Loss: 0.6589 Validation Loss: 0.6701\n",
      "282/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6701\n",
      "283/1000:\n",
      "Training Loss: 0.6617 Validation Loss: 0.6701\n",
      "284/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6700\n",
      "285/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6700\n",
      "286/1000:\n",
      "Training Loss: 0.6593 Validation Loss: 0.6700\n",
      "287/1000:\n",
      "Training Loss: 0.6586 Validation Loss: 0.6699\n",
      "288/1000:\n",
      "Training Loss: 0.6627 Validation Loss: 0.6699\n",
      "289/1000:\n",
      "Training Loss: 0.6593 Validation Loss: 0.6698\n",
      "290/1000:\n",
      "Training Loss: 0.6631 Validation Loss: 0.6698\n",
      "291/1000:\n",
      "Training Loss: 0.6584 Validation Loss: 0.6697\n",
      "292/1000:\n",
      "Training Loss: 0.6595 Validation Loss: 0.6697\n",
      "293/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6696\n",
      "294/1000:\n",
      "Training Loss: 0.6605 Validation Loss: 0.6696\n",
      "295/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6696\n",
      "296/1000:\n",
      "Training Loss: 0.6603 Validation Loss: 0.6695\n",
      "297/1000:\n",
      "Training Loss: 0.6577 Validation Loss: 0.6695\n",
      "298/1000:\n",
      "Training Loss: 0.6613 Validation Loss: 0.6694\n",
      "299/1000:\n",
      "Training Loss: 0.6624 Validation Loss: 0.6693\n",
      "300/1000:\n",
      "Training Loss: 0.6579 Validation Loss: 0.6693\n",
      "301/1000:\n",
      "Training Loss: 0.6596 Validation Loss: 0.6692\n",
      "302/1000:\n",
      "Training Loss: 0.6616 Validation Loss: 0.6692\n",
      "303/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6691\n",
      "304/1000:\n",
      "Training Loss: 0.6614 Validation Loss: 0.6691\n",
      "305/1000:\n",
      "Training Loss: 0.6608 Validation Loss: 0.6690\n",
      "306/1000:\n",
      "Training Loss: 0.6622 Validation Loss: 0.6690\n",
      "307/1000:\n",
      "Training Loss: 0.6592 Validation Loss: 0.6689\n",
      "308/1000:\n",
      "Training Loss: 0.6575 Validation Loss: 0.6689\n",
      "309/1000:\n",
      "Training Loss: 0.6582 Validation Loss: 0.6688\n",
      "310/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6688\n",
      "311/1000:\n",
      "Training Loss: 0.6573 Validation Loss: 0.6687\n",
      "312/1000:\n",
      "Training Loss: 0.6591 Validation Loss: 0.6686\n",
      "313/1000:\n",
      "Training Loss: 0.6589 Validation Loss: 0.6686\n",
      "314/1000:\n",
      "Training Loss: 0.6599 Validation Loss: 0.6685\n",
      "315/1000:\n",
      "Training Loss: 0.6620 Validation Loss: 0.6685\n",
      "316/1000:\n",
      "Training Loss: 0.6582 Validation Loss: 0.6684\n",
      "317/1000:\n",
      "Training Loss: 0.6592 Validation Loss: 0.6683\n",
      "318/1000:\n",
      "Training Loss: 0.6602 Validation Loss: 0.6683\n",
      "319/1000:\n",
      "Training Loss: 0.6581 Validation Loss: 0.6682\n",
      "320/1000:\n",
      "Training Loss: 0.6580 Validation Loss: 0.6681\n",
      "321/1000:\n",
      "Training Loss: 0.6596 Validation Loss: 0.6680\n",
      "322/1000:\n",
      "Training Loss: 0.6566 Validation Loss: 0.6680\n",
      "323/1000:\n",
      "Training Loss: 0.6550 Validation Loss: 0.6679\n",
      "324/1000:\n",
      "Training Loss: 0.6579 Validation Loss: 0.6678\n",
      "325/1000:\n",
      "Training Loss: 0.6594 Validation Loss: 0.6678\n",
      "326/1000:\n",
      "Training Loss: 0.6576 Validation Loss: 0.6677\n",
      "327/1000:\n",
      "Training Loss: 0.6575 Validation Loss: 0.6676\n",
      "328/1000:\n",
      "Training Loss: 0.6606 Validation Loss: 0.6675\n",
      "329/1000:\n",
      "Training Loss: 0.6582 Validation Loss: 0.6674\n",
      "330/1000:\n",
      "Training Loss: 0.6611 Validation Loss: 0.6673\n",
      "331/1000:\n",
      "Training Loss: 0.6587 Validation Loss: 0.6672\n",
      "332/1000:\n",
      "Training Loss: 0.6569 Validation Loss: 0.6672\n",
      "333/1000:\n",
      "Training Loss: 0.6567 Validation Loss: 0.6671\n",
      "334/1000:\n",
      "Training Loss: 0.6581 Validation Loss: 0.6670\n",
      "335/1000:\n",
      "Training Loss: 0.6576 Validation Loss: 0.6669\n",
      "336/1000:\n",
      "Training Loss: 0.6569 Validation Loss: 0.6668\n",
      "337/1000:\n",
      "Training Loss: 0.6579 Validation Loss: 0.6667\n",
      "338/1000:\n",
      "Training Loss: 0.6572 Validation Loss: 0.6666\n",
      "339/1000:\n",
      "Training Loss: 0.6565 Validation Loss: 0.6665\n",
      "340/1000:\n",
      "Training Loss: 0.6600 Validation Loss: 0.6664\n",
      "341/1000:\n",
      "Training Loss: 0.6587 Validation Loss: 0.6663\n",
      "342/1000:\n",
      "Training Loss: 0.6586 Validation Loss: 0.6662\n",
      "343/1000:\n",
      "Training Loss: 0.6564 Validation Loss: 0.6661\n",
      "344/1000:\n",
      "Training Loss: 0.6572 Validation Loss: 0.6660\n",
      "345/1000:\n",
      "Training Loss: 0.6581 Validation Loss: 0.6659\n",
      "346/1000:\n",
      "Training Loss: 0.6561 Validation Loss: 0.6658\n",
      "347/1000:\n",
      "Training Loss: 0.6572 Validation Loss: 0.6657\n",
      "348/1000:\n",
      "Training Loss: 0.6564 Validation Loss: 0.6656\n",
      "349/1000:\n",
      "Training Loss: 0.6552 Validation Loss: 0.6655\n",
      "350/1000:\n",
      "Training Loss: 0.6566 Validation Loss: 0.6654\n",
      "351/1000:\n",
      "Training Loss: 0.6554 Validation Loss: 0.6652\n",
      "352/1000:\n",
      "Training Loss: 0.6589 Validation Loss: 0.6651\n",
      "353/1000:\n",
      "Training Loss: 0.6551 Validation Loss: 0.6650\n",
      "354/1000:\n",
      "Training Loss: 0.6529 Validation Loss: 0.6649\n",
      "355/1000:\n",
      "Training Loss: 0.6577 Validation Loss: 0.6647\n",
      "356/1000:\n",
      "Training Loss: 0.6550 Validation Loss: 0.6646\n",
      "357/1000:\n",
      "Training Loss: 0.6559 Validation Loss: 0.6645\n",
      "358/1000:\n",
      "Training Loss: 0.6550 Validation Loss: 0.6643\n",
      "359/1000:\n",
      "Training Loss: 0.6553 Validation Loss: 0.6642\n",
      "360/1000:\n",
      "Training Loss: 0.6583 Validation Loss: 0.6641\n",
      "361/1000:\n",
      "Training Loss: 0.6566 Validation Loss: 0.6640\n",
      "362/1000:\n",
      "Training Loss: 0.6542 Validation Loss: 0.6638\n",
      "363/1000:\n",
      "Training Loss: 0.6549 Validation Loss: 0.6636\n",
      "364/1000:\n",
      "Training Loss: 0.6573 Validation Loss: 0.6635\n",
      "365/1000:\n",
      "Training Loss: 0.6561 Validation Loss: 0.6634\n",
      "366/1000:\n",
      "Training Loss: 0.6550 Validation Loss: 0.6632\n",
      "367/1000:\n",
      "Training Loss: 0.6556 Validation Loss: 0.6630\n",
      "368/1000:\n",
      "Training Loss: 0.6557 Validation Loss: 0.6629\n",
      "369/1000:\n",
      "Training Loss: 0.6513 Validation Loss: 0.6627\n",
      "370/1000:\n",
      "Training Loss: 0.6524 Validation Loss: 0.6625\n",
      "371/1000:\n",
      "Training Loss: 0.6522 Validation Loss: 0.6623\n",
      "372/1000:\n",
      "Training Loss: 0.6561 Validation Loss: 0.6622\n",
      "373/1000:\n",
      "Training Loss: 0.6531 Validation Loss: 0.6620\n",
      "374/1000:\n",
      "Training Loss: 0.6512 Validation Loss: 0.6618\n",
      "375/1000:\n",
      "Training Loss: 0.6541 Validation Loss: 0.6616\n",
      "376/1000:\n",
      "Training Loss: 0.6542 Validation Loss: 0.6614\n",
      "377/1000:\n",
      "Training Loss: 0.6509 Validation Loss: 0.6612\n",
      "378/1000:\n",
      "Training Loss: 0.6547 Validation Loss: 0.6610\n",
      "379/1000:\n",
      "Training Loss: 0.6496 Validation Loss: 0.6608\n",
      "380/1000:\n",
      "Training Loss: 0.6524 Validation Loss: 0.6606\n",
      "381/1000:\n",
      "Training Loss: 0.6493 Validation Loss: 0.6604\n",
      "382/1000:\n",
      "Training Loss: 0.6530 Validation Loss: 0.6602\n",
      "383/1000:\n",
      "Training Loss: 0.6501 Validation Loss: 0.6599\n",
      "384/1000:\n",
      "Training Loss: 0.6508 Validation Loss: 0.6597\n",
      "385/1000:\n",
      "Training Loss: 0.6521 Validation Loss: 0.6595\n",
      "386/1000:\n",
      "Training Loss: 0.6512 Validation Loss: 0.6593\n",
      "387/1000:\n",
      "Training Loss: 0.6508 Validation Loss: 0.6590\n",
      "388/1000:\n",
      "Training Loss: 0.6542 Validation Loss: 0.6588\n",
      "389/1000:\n",
      "Training Loss: 0.6532 Validation Loss: 0.6585\n",
      "390/1000:\n",
      "Training Loss: 0.6494 Validation Loss: 0.6583\n",
      "391/1000:\n",
      "Training Loss: 0.6500 Validation Loss: 0.6580\n",
      "392/1000:\n",
      "Training Loss: 0.6488 Validation Loss: 0.6577\n",
      "393/1000:\n",
      "Training Loss: 0.6528 Validation Loss: 0.6575\n",
      "394/1000:\n",
      "Training Loss: 0.6460 Validation Loss: 0.6572\n",
      "395/1000:\n",
      "Training Loss: 0.6501 Validation Loss: 0.6569\n",
      "396/1000:\n",
      "Training Loss: 0.6478 Validation Loss: 0.6566\n",
      "397/1000:\n",
      "Training Loss: 0.6518 Validation Loss: 0.6563\n",
      "398/1000:\n",
      "Training Loss: 0.6500 Validation Loss: 0.6560\n",
      "399/1000:\n",
      "Training Loss: 0.6475 Validation Loss: 0.6557\n",
      "400/1000:\n",
      "Training Loss: 0.6488 Validation Loss: 0.6554\n",
      "401/1000:\n",
      "Training Loss: 0.6513 Validation Loss: 0.6551\n",
      "402/1000:\n",
      "Training Loss: 0.6479 Validation Loss: 0.6547\n",
      "403/1000:\n",
      "Training Loss: 0.6458 Validation Loss: 0.6544\n",
      "404/1000:\n",
      "Training Loss: 0.6497 Validation Loss: 0.6541\n",
      "405/1000:\n",
      "Training Loss: 0.6496 Validation Loss: 0.6537\n",
      "406/1000:\n",
      "Training Loss: 0.6498 Validation Loss: 0.6534\n",
      "407/1000:\n",
      "Training Loss: 0.6460 Validation Loss: 0.6530\n",
      "408/1000:\n",
      "Training Loss: 0.6515 Validation Loss: 0.6526\n",
      "409/1000:\n",
      "Training Loss: 0.6478 Validation Loss: 0.6523\n",
      "410/1000:\n",
      "Training Loss: 0.6412 Validation Loss: 0.6518\n",
      "411/1000:\n",
      "Training Loss: 0.6449 Validation Loss: 0.6514\n",
      "412/1000:\n",
      "Training Loss: 0.6448 Validation Loss: 0.6510\n",
      "413/1000:\n",
      "Training Loss: 0.6472 Validation Loss: 0.6506\n",
      "414/1000:\n",
      "Training Loss: 0.6457 Validation Loss: 0.6502\n",
      "415/1000:\n",
      "Training Loss: 0.6475 Validation Loss: 0.6498\n",
      "416/1000:\n",
      "Training Loss: 0.6409 Validation Loss: 0.6493\n",
      "417/1000:\n",
      "Training Loss: 0.6449 Validation Loss: 0.6488\n",
      "418/1000:\n",
      "Training Loss: 0.6451 Validation Loss: 0.6484\n",
      "419/1000:\n",
      "Training Loss: 0.6403 Validation Loss: 0.6479\n",
      "420/1000:\n",
      "Training Loss: 0.6432 Validation Loss: 0.6474\n",
      "421/1000:\n",
      "Training Loss: 0.6399 Validation Loss: 0.6469\n",
      "422/1000:\n",
      "Training Loss: 0.6387 Validation Loss: 0.6463\n",
      "423/1000:\n",
      "Training Loss: 0.6330 Validation Loss: 0.6457\n",
      "424/1000:\n",
      "Training Loss: 0.6346 Validation Loss: 0.6451\n",
      "425/1000:\n",
      "Training Loss: 0.6408 Validation Loss: 0.6445\n",
      "426/1000:\n",
      "Training Loss: 0.6370 Validation Loss: 0.6439\n",
      "427/1000:\n",
      "Training Loss: 0.6360 Validation Loss: 0.6433\n",
      "428/1000:\n",
      "Training Loss: 0.6338 Validation Loss: 0.6426\n",
      "429/1000:\n",
      "Training Loss: 0.6373 Validation Loss: 0.6420\n",
      "430/1000:\n",
      "Training Loss: 0.6359 Validation Loss: 0.6414\n",
      "431/1000:\n",
      "Training Loss: 0.6314 Validation Loss: 0.6406\n",
      "432/1000:\n",
      "Training Loss: 0.6398 Validation Loss: 0.6400\n",
      "433/1000:\n",
      "Training Loss: 0.6325 Validation Loss: 0.6393\n",
      "434/1000:\n",
      "Training Loss: 0.6374 Validation Loss: 0.6387\n",
      "435/1000:\n",
      "Training Loss: 0.6338 Validation Loss: 0.6379\n",
      "436/1000:\n",
      "Training Loss: 0.6384 Validation Loss: 0.6373\n",
      "437/1000:\n",
      "Training Loss: 0.6298 Validation Loss: 0.6365\n",
      "438/1000:\n",
      "Training Loss: 0.6371 Validation Loss: 0.6358\n",
      "439/1000:\n",
      "Training Loss: 0.6285 Validation Loss: 0.6350\n",
      "440/1000:\n",
      "Training Loss: 0.6304 Validation Loss: 0.6342\n",
      "441/1000:\n",
      "Training Loss: 0.6353 Validation Loss: 0.6335\n",
      "442/1000:\n",
      "Training Loss: 0.6307 Validation Loss: 0.6327\n",
      "443/1000:\n",
      "Training Loss: 0.6273 Validation Loss: 0.6318\n",
      "444/1000:\n",
      "Training Loss: 0.6324 Validation Loss: 0.6310\n",
      "445/1000:\n",
      "Training Loss: 0.6301 Validation Loss: 0.6301\n",
      "446/1000:\n",
      "Training Loss: 0.6265 Validation Loss: 0.6293\n",
      "447/1000:\n",
      "Training Loss: 0.6306 Validation Loss: 0.6285\n",
      "448/1000:\n",
      "Training Loss: 0.6329 Validation Loss: 0.6277\n",
      "449/1000:\n",
      "Training Loss: 0.6336 Validation Loss: 0.6270\n",
      "450/1000:\n",
      "Training Loss: 0.6289 Validation Loss: 0.6262\n",
      "451/1000:\n",
      "Training Loss: 0.6271 Validation Loss: 0.6253\n",
      "452/1000:\n",
      "Training Loss: 0.6273 Validation Loss: 0.6245\n",
      "453/1000:\n",
      "Training Loss: 0.6274 Validation Loss: 0.6236\n",
      "454/1000:\n",
      "Training Loss: 0.6351 Validation Loss: 0.6228\n",
      "455/1000:\n",
      "Training Loss: 0.6276 Validation Loss: 0.6219\n",
      "456/1000:\n",
      "Training Loss: 0.6219 Validation Loss: 0.6210\n",
      "457/1000:\n",
      "Training Loss: 0.6205 Validation Loss: 0.6200\n",
      "458/1000:\n",
      "Training Loss: 0.6306 Validation Loss: 0.6192\n",
      "459/1000:\n",
      "Training Loss: 0.6226 Validation Loss: 0.6183\n",
      "460/1000:\n",
      "Training Loss: 0.6252 Validation Loss: 0.6174\n",
      "461/1000:\n",
      "Training Loss: 0.6271 Validation Loss: 0.6165\n",
      "462/1000:\n",
      "Training Loss: 0.6274 Validation Loss: 0.6158\n",
      "463/1000:\n",
      "Training Loss: 0.6188 Validation Loss: 0.6148\n",
      "464/1000:\n",
      "Training Loss: 0.6122 Validation Loss: 0.6137\n",
      "465/1000:\n",
      "Training Loss: 0.6248 Validation Loss: 0.6130\n",
      "466/1000:\n",
      "Training Loss: 0.6165 Validation Loss: 0.6120\n",
      "467/1000:\n",
      "Training Loss: 0.6090 Validation Loss: 0.6107\n",
      "468/1000:\n",
      "Training Loss: 0.6087 Validation Loss: 0.6095\n",
      "469/1000:\n",
      "Training Loss: 0.6098 Validation Loss: 0.6085\n",
      "470/1000:\n",
      "Training Loss: 0.6188 Validation Loss: 0.6076\n",
      "471/1000:\n",
      "Training Loss: 0.6144 Validation Loss: 0.6066\n",
      "472/1000:\n",
      "Training Loss: 0.6095 Validation Loss: 0.6055\n",
      "473/1000:\n",
      "Training Loss: 0.6228 Validation Loss: 0.6048\n",
      "474/1000:\n",
      "Training Loss: 0.6140 Validation Loss: 0.6039\n",
      "475/1000:\n",
      "Training Loss: 0.6131 Validation Loss: 0.6030\n",
      "476/1000:\n",
      "Training Loss: 0.6136 Validation Loss: 0.6020\n",
      "477/1000:\n",
      "Training Loss: 0.6165 Validation Loss: 0.6012\n",
      "478/1000:\n",
      "Training Loss: 0.6244 Validation Loss: 0.6007\n",
      "479/1000:\n",
      "Training Loss: 0.6165 Validation Loss: 0.5999\n",
      "480/1000:\n",
      "Training Loss: 0.6063 Validation Loss: 0.5989\n",
      "481/1000:\n",
      "Training Loss: 0.6062 Validation Loss: 0.5978\n",
      "482/1000:\n",
      "Training Loss: 0.6108 Validation Loss: 0.5968\n",
      "483/1000:\n",
      "Training Loss: 0.6115 Validation Loss: 0.5961\n",
      "484/1000:\n",
      "Training Loss: 0.6094 Validation Loss: 0.5950\n",
      "485/1000:\n",
      "Training Loss: 0.6048 Validation Loss: 0.5941\n",
      "486/1000:\n",
      "Training Loss: 0.6113 Validation Loss: 0.5933\n",
      "487/1000:\n",
      "Training Loss: 0.6188 Validation Loss: 0.5927\n",
      "488/1000:\n",
      "Training Loss: 0.6091 Validation Loss: 0.5919\n",
      "489/1000:\n",
      "Training Loss: 0.6106 Validation Loss: 0.5912\n",
      "490/1000:\n",
      "Training Loss: 0.6051 Validation Loss: 0.5902\n",
      "491/1000:\n",
      "Training Loss: 0.6088 Validation Loss: 0.5895\n",
      "492/1000:\n",
      "Training Loss: 0.6112 Validation Loss: 0.5887\n",
      "493/1000:\n",
      "Training Loss: 0.6072 Validation Loss: 0.5879\n",
      "494/1000:\n",
      "Training Loss: 0.6079 Validation Loss: 0.5873\n",
      "495/1000:\n",
      "Training Loss: 0.6108 Validation Loss: 0.5867\n",
      "496/1000:\n",
      "Training Loss: 0.5944 Validation Loss: 0.5855\n",
      "497/1000:\n",
      "Training Loss: 0.6004 Validation Loss: 0.5847\n",
      "498/1000:\n",
      "Training Loss: 0.6004 Validation Loss: 0.5838\n",
      "499/1000:\n",
      "Training Loss: 0.6012 Validation Loss: 0.5828\n",
      "500/1000:\n",
      "Training Loss: 0.6085 Validation Loss: 0.5822\n",
      "501/1000:\n",
      "Training Loss: 0.5991 Validation Loss: 0.5813\n",
      "502/1000:\n",
      "Training Loss: 0.5940 Validation Loss: 0.5802\n",
      "503/1000:\n",
      "Training Loss: 0.6045 Validation Loss: 0.5797\n",
      "504/1000:\n",
      "Training Loss: 0.5936 Validation Loss: 0.5786\n",
      "505/1000:\n",
      "Training Loss: 0.6105 Validation Loss: 0.5783\n",
      "506/1000:\n",
      "Training Loss: 0.5918 Validation Loss: 0.5775\n",
      "507/1000:\n",
      "Training Loss: 0.5963 Validation Loss: 0.5767\n",
      "508/1000:\n",
      "Training Loss: 0.5911 Validation Loss: 0.5756\n",
      "509/1000:\n",
      "Training Loss: 0.5958 Validation Loss: 0.5749\n",
      "510/1000:\n",
      "Training Loss: 0.5974 Validation Loss: 0.5742\n",
      "511/1000:\n",
      "Training Loss: 0.5961 Validation Loss: 0.5733\n",
      "512/1000:\n",
      "Training Loss: 0.6083 Validation Loss: 0.5730\n",
      "513/1000:\n",
      "Training Loss: 0.6011 Validation Loss: 0.5723\n",
      "514/1000:\n",
      "Training Loss: 0.6090 Validation Loss: 0.5722\n",
      "515/1000:\n",
      "Training Loss: 0.6127 Validation Loss: 0.5722\n",
      "516/1000:\n",
      "Training Loss: 0.6016 Validation Loss: 0.5716\n",
      "517/1000:\n",
      "Training Loss: 0.5947 Validation Loss: 0.5710\n",
      "518/1000:\n",
      "Training Loss: 0.5928 Validation Loss: 0.5702\n",
      "519/1000:\n",
      "Training Loss: 0.5968 Validation Loss: 0.5694\n",
      "520/1000:\n",
      "Training Loss: 0.5904 Validation Loss: 0.5685\n",
      "521/1000:\n",
      "Training Loss: 0.6013 Validation Loss: 0.5680\n",
      "522/1000:\n",
      "Training Loss: 0.5961 Validation Loss: 0.5673\n",
      "523/1000:\n",
      "Training Loss: 0.5878 Validation Loss: 0.5667\n",
      "524/1000:\n",
      "Training Loss: 0.5977 Validation Loss: 0.5664\n",
      "525/1000:\n",
      "Training Loss: 0.6008 Validation Loss: 0.5662\n",
      "526/1000:\n",
      "Training Loss: 0.6039 Validation Loss: 0.5660\n",
      "527/1000:\n",
      "Training Loss: 0.6002 Validation Loss: 0.5657\n",
      "528/1000:\n",
      "Training Loss: 0.5801 Validation Loss: 0.5647\n",
      "529/1000:\n",
      "Training Loss: 0.5927 Validation Loss: 0.5640\n",
      "530/1000:\n",
      "Training Loss: 0.6052 Validation Loss: 0.5641\n",
      "531/1000:\n",
      "Training Loss: 0.5913 Validation Loss: 0.5632\n",
      "532/1000:\n",
      "Training Loss: 0.6005 Validation Loss: 0.5631\n",
      "533/1000:\n",
      "Training Loss: 0.6005 Validation Loss: 0.5627\n",
      "534/1000:\n",
      "Training Loss: 0.5972 Validation Loss: 0.5624\n",
      "535/1000:\n",
      "Training Loss: 0.6007 Validation Loss: 0.5616\n",
      "536/1000:\n",
      "Training Loss: 0.6027 Validation Loss: 0.5616\n",
      "537/1000:\n",
      "Training Loss: 0.5879 Validation Loss: 0.5612\n",
      "538/1000:\n",
      "Training Loss: 0.5981 Validation Loss: 0.5608\n",
      "539/1000:\n",
      "Training Loss: 0.5917 Validation Loss: 0.5604\n",
      "540/1000:\n",
      "Training Loss: 0.5793 Validation Loss: 0.5593\n",
      "541/1000:\n",
      "Training Loss: 0.5827 Validation Loss: 0.5583\n",
      "542/1000:\n",
      "Training Loss: 0.5837 Validation Loss: 0.5573\n",
      "543/1000:\n",
      "Training Loss: 0.5782 Validation Loss: 0.5562\n",
      "544/1000:\n",
      "Training Loss: 0.5887 Validation Loss: 0.5559\n",
      "545/1000:\n",
      "Training Loss: 0.5910 Validation Loss: 0.5558\n",
      "546/1000:\n",
      "Training Loss: 0.5884 Validation Loss: 0.5553\n",
      "547/1000:\n",
      "Training Loss: 0.5777 Validation Loss: 0.5541\n",
      "548/1000:\n",
      "Training Loss: 0.5799 Validation Loss: 0.5536\n",
      "549/1000:\n",
      "Training Loss: 0.5894 Validation Loss: 0.5538\n",
      "550/1000:\n",
      "Training Loss: 0.5940 Validation Loss: 0.5533\n",
      "551/1000:\n",
      "Training Loss: 0.5801 Validation Loss: 0.5528\n",
      "552/1000:\n",
      "Training Loss: 0.5930 Validation Loss: 0.5531\n",
      "553/1000:\n",
      "Training Loss: 0.5885 Validation Loss: 0.5522\n",
      "554/1000:\n",
      "Training Loss: 0.5685 Validation Loss: 0.5510\n",
      "555/1000:\n",
      "Training Loss: 0.5819 Validation Loss: 0.5502\n",
      "556/1000:\n",
      "Training Loss: 0.5922 Validation Loss: 0.5502\n",
      "557/1000:\n",
      "Training Loss: 0.5831 Validation Loss: 0.5499\n",
      "558/1000:\n",
      "Training Loss: 0.6020 Validation Loss: 0.5499\n",
      "559/1000:\n",
      "Training Loss: 0.5851 Validation Loss: 0.5495\n",
      "560/1000:\n",
      "Training Loss: 0.5836 Validation Loss: 0.5489\n",
      "561/1000:\n",
      "Training Loss: 0.5895 Validation Loss: 0.5490\n",
      "562/1000:\n",
      "Training Loss: 0.5713 Validation Loss: 0.5488\n",
      "563/1000:\n",
      "Training Loss: 0.5890 Validation Loss: 0.5488\n",
      "564/1000:\n",
      "Training Loss: 0.5566 Validation Loss: 0.5470\n",
      "565/1000:\n",
      "Training Loss: 0.5819 Validation Loss: 0.5466\n",
      "566/1000:\n",
      "Training Loss: 0.5801 Validation Loss: 0.5465\n",
      "567/1000:\n",
      "Training Loss: 0.5963 Validation Loss: 0.5466\n",
      "568/1000:\n",
      "Training Loss: 0.5655 Validation Loss: 0.5454\n",
      "569/1000:\n",
      "Training Loss: 0.5729 Validation Loss: 0.5447\n",
      "570/1000:\n",
      "Training Loss: 0.5739 Validation Loss: 0.5440\n",
      "571/1000:\n",
      "Training Loss: 0.6022 Validation Loss: 0.5448\n",
      "572/1000:\n",
      "Training Loss: 0.5877 Validation Loss: 0.5444\n",
      "573/1000:\n",
      "Training Loss: 0.5924 Validation Loss: 0.5448\n",
      "574/1000:\n",
      "Training Loss: 0.5803 Validation Loss: 0.5445\n",
      "575/1000:\n",
      "Training Loss: 0.5787 Validation Loss: 0.5443\n",
      "576/1000:\n",
      "Training Loss: 0.5779 Validation Loss: 0.5436\n",
      "577/1000:\n",
      "Training Loss: 0.5824 Validation Loss: 0.5431\n",
      "578/1000:\n",
      "Training Loss: 0.5773 Validation Loss: 0.5428\n",
      "579/1000:\n",
      "Training Loss: 0.5877 Validation Loss: 0.5433\n",
      "580/1000:\n",
      "Training Loss: 0.5710 Validation Loss: 0.5424\n",
      "581/1000:\n",
      "Training Loss: 0.5679 Validation Loss: 0.5413\n",
      "582/1000:\n",
      "Training Loss: 0.5852 Validation Loss: 0.5415\n",
      "583/1000:\n",
      "Training Loss: 0.5682 Validation Loss: 0.5406\n",
      "584/1000:\n",
      "Training Loss: 0.5658 Validation Loss: 0.5402\n",
      "585/1000:\n",
      "Training Loss: 0.5784 Validation Loss: 0.5401\n",
      "586/1000:\n",
      "Training Loss: 0.5601 Validation Loss: 0.5384\n",
      "587/1000:\n",
      "Training Loss: 0.5926 Validation Loss: 0.5389\n",
      "588/1000:\n",
      "Training Loss: 0.5695 Validation Loss: 0.5380\n",
      "589/1000:\n",
      "Training Loss: 0.5795 Validation Loss: 0.5385\n",
      "590/1000:\n",
      "Training Loss: 0.5680 Validation Loss: 0.5385\n",
      "591/1000:\n",
      "Training Loss: 0.5793 Validation Loss: 0.5383\n",
      "592/1000:\n",
      "Training Loss: 0.5701 Validation Loss: 0.5377\n",
      "593/1000:\n",
      "Training Loss: 0.5893 Validation Loss: 0.5377\n",
      "594/1000:\n",
      "Training Loss: 0.5743 Validation Loss: 0.5372\n",
      "595/1000:\n",
      "Training Loss: 0.5635 Validation Loss: 0.5362\n",
      "596/1000:\n",
      "Training Loss: 0.5844 Validation Loss: 0.5362\n",
      "597/1000:\n",
      "Training Loss: 0.5713 Validation Loss: 0.5354\n",
      "598/1000:\n",
      "Training Loss: 0.5683 Validation Loss: 0.5355\n",
      "599/1000:\n",
      "Training Loss: 0.5731 Validation Loss: 0.5348\n",
      "600/1000:\n",
      "Training Loss: 0.5837 Validation Loss: 0.5354\n",
      "601/1000:\n",
      "Training Loss: 0.5772 Validation Loss: 0.5349\n",
      "602/1000:\n",
      "Training Loss: 0.5765 Validation Loss: 0.5345\n",
      "603/1000:\n",
      "Training Loss: 0.5644 Validation Loss: 0.5345\n",
      "604/1000:\n",
      "Training Loss: 0.5839 Validation Loss: 0.5348\n",
      "605/1000:\n",
      "Training Loss: 0.5858 Validation Loss: 0.5349\n",
      "606/1000:\n",
      "Training Loss: 0.5556 Validation Loss: 0.5333\n",
      "607/1000:\n",
      "Training Loss: 0.5811 Validation Loss: 0.5335\n",
      "608/1000:\n",
      "Training Loss: 0.5625 Validation Loss: 0.5326\n",
      "609/1000:\n",
      "Training Loss: 0.5594 Validation Loss: 0.5320\n",
      "610/1000:\n",
      "Training Loss: 0.5688 Validation Loss: 0.5316\n",
      "611/1000:\n",
      "Training Loss: 0.5967 Validation Loss: 0.5330\n",
      "612/1000:\n",
      "Training Loss: 0.5829 Validation Loss: 0.5332\n",
      "613/1000:\n",
      "Training Loss: 0.5519 Validation Loss: 0.5324\n",
      "614/1000:\n",
      "Training Loss: 0.5777 Validation Loss: 0.5319\n",
      "615/1000:\n",
      "Training Loss: 0.5698 Validation Loss: 0.5314\n",
      "616/1000:\n",
      "Training Loss: 0.5819 Validation Loss: 0.5312\n",
      "617/1000:\n",
      "Training Loss: 0.5752 Validation Loss: 0.5308\n",
      "618/1000:\n",
      "Training Loss: 0.5559 Validation Loss: 0.5306\n",
      "619/1000:\n",
      "Training Loss: 0.5808 Validation Loss: 0.5307\n",
      "620/1000:\n",
      "Training Loss: 0.5782 Validation Loss: 0.5308\n",
      "621/1000:\n",
      "Training Loss: 0.5703 Validation Loss: 0.5304\n",
      "622/1000:\n",
      "Training Loss: 0.5927 Validation Loss: 0.5311\n",
      "623/1000:\n",
      "Training Loss: 0.5688 Validation Loss: 0.5301\n",
      "624/1000:\n",
      "Training Loss: 0.5774 Validation Loss: 0.5305\n",
      "625/1000:\n",
      "Training Loss: 0.5680 Validation Loss: 0.5303\n",
      "626/1000:\n",
      "Training Loss: 0.5834 Validation Loss: 0.5305\n",
      "627/1000:\n",
      "Training Loss: 0.5817 Validation Loss: 0.5305\n",
      "628/1000:\n",
      "Training Loss: 0.5661 Validation Loss: 0.5303\n",
      "629/1000:\n",
      "Training Loss: 0.5666 Validation Loss: 0.5298\n",
      "630/1000:\n",
      "Training Loss: 0.5683 Validation Loss: 0.5294\n",
      "631/1000:\n",
      "Training Loss: 0.5668 Validation Loss: 0.5286\n",
      "632/1000:\n",
      "Training Loss: 0.5846 Validation Loss: 0.5287\n",
      "633/1000:\n",
      "Training Loss: 0.5735 Validation Loss: 0.5285\n",
      "634/1000:\n",
      "Training Loss: 0.5777 Validation Loss: 0.5285\n",
      "635/1000:\n",
      "Training Loss: 0.5815 Validation Loss: 0.5288\n",
      "636/1000:\n",
      "Training Loss: 0.5747 Validation Loss: 0.5285\n",
      "637/1000:\n",
      "Training Loss: 0.5696 Validation Loss: 0.5284\n",
      "638/1000:\n",
      "Training Loss: 0.5557 Validation Loss: 0.5274\n",
      "639/1000:\n",
      "Training Loss: 0.5873 Validation Loss: 0.5277\n",
      "640/1000:\n",
      "Training Loss: 0.5655 Validation Loss: 0.5273\n",
      "641/1000:\n",
      "Training Loss: 0.5634 Validation Loss: 0.5269\n",
      "642/1000:\n",
      "Training Loss: 0.5835 Validation Loss: 0.5277\n",
      "643/1000:\n",
      "Training Loss: 0.5463 Validation Loss: 0.5262\n",
      "644/1000:\n",
      "Training Loss: 0.5634 Validation Loss: 0.5251\n",
      "645/1000:\n",
      "Training Loss: 0.5652 Validation Loss: 0.5247\n",
      "646/1000:\n",
      "Training Loss: 0.5750 Validation Loss: 0.5245\n",
      "647/1000:\n",
      "Training Loss: 0.5802 Validation Loss: 0.5253\n",
      "648/1000:\n",
      "Training Loss: 0.5649 Validation Loss: 0.5250\n",
      "649/1000:\n",
      "Training Loss: 0.5816 Validation Loss: 0.5257\n",
      "650/1000:\n",
      "Training Loss: 0.5630 Validation Loss: 0.5255\n",
      "651/1000:\n",
      "Training Loss: 0.5725 Validation Loss: 0.5254\n",
      "652/1000:\n",
      "Training Loss: 0.5457 Validation Loss: 0.5240\n",
      "653/1000:\n",
      "Training Loss: 0.5646 Validation Loss: 0.5235\n",
      "654/1000:\n",
      "Training Loss: 0.5598 Validation Loss: 0.5231\n",
      "655/1000:\n",
      "Training Loss: 0.5618 Validation Loss: 0.5224\n",
      "656/1000:\n",
      "Training Loss: 0.5615 Validation Loss: 0.5219\n",
      "657/1000:\n",
      "Training Loss: 0.5633 Validation Loss: 0.5218\n",
      "658/1000:\n",
      "Training Loss: 0.5552 Validation Loss: 0.5213\n",
      "659/1000:\n",
      "Training Loss: 0.5760 Validation Loss: 0.5214\n",
      "660/1000:\n",
      "Training Loss: 0.5828 Validation Loss: 0.5226\n",
      "661/1000:\n",
      "Training Loss: 0.5603 Validation Loss: 0.5221\n",
      "662/1000:\n",
      "Training Loss: 0.5771 Validation Loss: 0.5226\n",
      "663/1000:\n",
      "Training Loss: 0.5641 Validation Loss: 0.5217\n",
      "664/1000:\n",
      "Training Loss: 0.5784 Validation Loss: 0.5224\n",
      "665/1000:\n",
      "Training Loss: 0.5750 Validation Loss: 0.5227\n",
      "666/1000:\n",
      "Training Loss: 0.5619 Validation Loss: 0.5223\n",
      "667/1000:\n",
      "Training Loss: 0.5820 Validation Loss: 0.5226\n",
      "668/1000:\n",
      "Training Loss: 0.5676 Validation Loss: 0.5224\n",
      "669/1000:\n",
      "Training Loss: 0.5627 Validation Loss: 0.5218\n",
      "670/1000:\n",
      "Training Loss: 0.5582 Validation Loss: 0.5209\n",
      "671/1000:\n",
      "Training Loss: 0.5717 Validation Loss: 0.5209\n",
      "672/1000:\n",
      "Training Loss: 0.5670 Validation Loss: 0.5203\n",
      "673/1000:\n",
      "Training Loss: 0.5559 Validation Loss: 0.5193\n",
      "674/1000:\n",
      "Training Loss: 0.5825 Validation Loss: 0.5203\n",
      "675/1000:\n",
      "Training Loss: 0.5776 Validation Loss: 0.5211\n",
      "676/1000:\n",
      "Training Loss: 0.5546 Validation Loss: 0.5198\n",
      "677/1000:\n",
      "Training Loss: 0.5629 Validation Loss: 0.5195\n",
      "678/1000:\n",
      "Training Loss: 0.5787 Validation Loss: 0.5202\n",
      "679/1000:\n",
      "Training Loss: 0.5605 Validation Loss: 0.5195\n",
      "680/1000:\n",
      "Training Loss: 0.5735 Validation Loss: 0.5199\n",
      "681/1000:\n",
      "Training Loss: 0.5530 Validation Loss: 0.5196\n",
      "682/1000:\n",
      "Training Loss: 0.5581 Validation Loss: 0.5187\n",
      "683/1000:\n",
      "Training Loss: 0.5605 Validation Loss: 0.5180\n",
      "684/1000:\n",
      "Training Loss: 0.5579 Validation Loss: 0.5174\n",
      "685/1000:\n",
      "Training Loss: 0.5755 Validation Loss: 0.5179\n",
      "686/1000:\n",
      "Training Loss: 0.5570 Validation Loss: 0.5177\n",
      "687/1000:\n",
      "Training Loss: 0.5512 Validation Loss: 0.5172\n",
      "688/1000:\n",
      "Training Loss: 0.5595 Validation Loss: 0.5167\n",
      "689/1000:\n",
      "Training Loss: 0.5571 Validation Loss: 0.5163\n",
      "690/1000:\n",
      "Training Loss: 0.5643 Validation Loss: 0.5156\n",
      "691/1000:\n",
      "Training Loss: 0.5603 Validation Loss: 0.5152\n",
      "692/1000:\n",
      "Training Loss: 0.5481 Validation Loss: 0.5145\n",
      "693/1000:\n",
      "Training Loss: 0.5650 Validation Loss: 0.5148\n",
      "694/1000:\n",
      "Training Loss: 0.5592 Validation Loss: 0.5141\n",
      "695/1000:\n",
      "Training Loss: 0.5582 Validation Loss: 0.5136\n",
      "696/1000:\n",
      "Training Loss: 0.5595 Validation Loss: 0.5134\n",
      "697/1000:\n",
      "Training Loss: 0.5854 Validation Loss: 0.5149\n",
      "698/1000:\n",
      "Training Loss: 0.5809 Validation Loss: 0.5158\n",
      "699/1000:\n",
      "Training Loss: 0.5631 Validation Loss: 0.5157\n",
      "700/1000:\n",
      "Training Loss: 0.5539 Validation Loss: 0.5147\n",
      "701/1000:\n",
      "Training Loss: 0.5608 Validation Loss: 0.5139\n",
      "702/1000:\n",
      "Training Loss: 0.5578 Validation Loss: 0.5136\n",
      "703/1000:\n",
      "Training Loss: 0.5629 Validation Loss: 0.5136\n",
      "704/1000:\n",
      "Training Loss: 0.5715 Validation Loss: 0.5145\n",
      "705/1000:\n",
      "Training Loss: 0.5690 Validation Loss: 0.5146\n",
      "706/1000:\n",
      "Training Loss: 0.5617 Validation Loss: 0.5147\n",
      "707/1000:\n",
      "Training Loss: 0.5609 Validation Loss: 0.5141\n",
      "708/1000:\n",
      "Training Loss: 0.5661 Validation Loss: 0.5148\n",
      "709/1000:\n",
      "Training Loss: 0.5639 Validation Loss: 0.5148\n",
      "710/1000:\n",
      "Training Loss: 0.5544 Validation Loss: 0.5148\n",
      "711/1000:\n",
      "Training Loss: 0.5609 Validation Loss: 0.5136\n",
      "712/1000:\n",
      "Training Loss: 0.5550 Validation Loss: 0.5141\n",
      "713/1000:\n",
      "Training Loss: 0.5734 Validation Loss: 0.5142\n",
      "714/1000:\n",
      "Training Loss: 0.5603 Validation Loss: 0.5140\n",
      "715/1000:\n",
      "Training Loss: 0.5572 Validation Loss: 0.5141\n",
      "716/1000:\n",
      "Training Loss: 0.5629 Validation Loss: 0.5140\n",
      "717/1000:\n",
      "Training Loss: 0.5700 Validation Loss: 0.5137\n",
      "718/1000:\n",
      "Training Loss: 0.5519 Validation Loss: 0.5128\n",
      "719/1000:\n",
      "Training Loss: 0.5689 Validation Loss: 0.5137\n",
      "720/1000:\n",
      "Training Loss: 0.5643 Validation Loss: 0.5131\n",
      "721/1000:\n",
      "Training Loss: 0.5698 Validation Loss: 0.5136\n",
      "722/1000:\n",
      "Training Loss: 0.5610 Validation Loss: 0.5131\n",
      "723/1000:\n",
      "Training Loss: 0.5627 Validation Loss: 0.5129\n",
      "724/1000:\n",
      "Training Loss: 0.5824 Validation Loss: 0.5134\n",
      "725/1000:\n",
      "Training Loss: 0.5645 Validation Loss: 0.5136\n",
      "726/1000:\n",
      "Training Loss: 0.5588 Validation Loss: 0.5130\n",
      "727/1000:\n",
      "Training Loss: 0.5990 Validation Loss: 0.5133\n",
      "728/1000:\n",
      "Training Loss: 0.5577 Validation Loss: 0.5128\n",
      "729/1000:\n",
      "Training Loss: 0.5464 Validation Loss: 0.5122\n",
      "730/1000:\n",
      "Training Loss: 0.5502 Validation Loss: 0.5117\n",
      "731/1000:\n",
      "Training Loss: 0.5531 Validation Loss: 0.5110\n",
      "732/1000:\n",
      "Training Loss: 0.5794 Validation Loss: 0.5119\n",
      "733/1000:\n",
      "Training Loss: 0.5739 Validation Loss: 0.5120\n",
      "734/1000:\n",
      "Training Loss: 0.5497 Validation Loss: 0.5117\n",
      "735/1000:\n",
      "Training Loss: 0.5660 Validation Loss: 0.5128\n",
      "736/1000:\n",
      "Training Loss: 0.5541 Validation Loss: 0.5125\n",
      "737/1000:\n",
      "Training Loss: 0.5433 Validation Loss: 0.5111\n",
      "738/1000:\n",
      "Training Loss: 0.5653 Validation Loss: 0.5114\n",
      "739/1000:\n",
      "Training Loss: 0.5690 Validation Loss: 0.5122\n",
      "740/1000:\n",
      "Training Loss: 0.5584 Validation Loss: 0.5119\n",
      "741/1000:\n",
      "Training Loss: 0.5554 Validation Loss: 0.5119\n",
      "742/1000:\n",
      "Training Loss: 0.5782 Validation Loss: 0.5118\n",
      "743/1000:\n",
      "Training Loss: 0.5489 Validation Loss: 0.5117\n",
      "744/1000:\n",
      "Training Loss: 0.5763 Validation Loss: 0.5127\n",
      "745/1000:\n",
      "Training Loss: 0.5706 Validation Loss: 0.5132\n",
      "746/1000:\n",
      "Training Loss: 0.5535 Validation Loss: 0.5130\n",
      "747/1000:\n",
      "Training Loss: 0.5762 Validation Loss: 0.5133\n",
      "748/1000:\n",
      "Training Loss: 0.5535 Validation Loss: 0.5123\n",
      "749/1000:\n",
      "Training Loss: 0.5564 Validation Loss: 0.5117\n",
      "750/1000:\n",
      "Training Loss: 0.5418 Validation Loss: 0.5109\n",
      "751/1000:\n",
      "Training Loss: 0.5610 Validation Loss: 0.5104\n",
      "752/1000:\n",
      "Training Loss: 0.5485 Validation Loss: 0.5100\n",
      "753/1000:\n",
      "Training Loss: 0.5895 Validation Loss: 0.5118\n",
      "754/1000:\n",
      "Training Loss: 0.5466 Validation Loss: 0.5106\n",
      "755/1000:\n",
      "Training Loss: 0.5431 Validation Loss: 0.5091\n",
      "756/1000:\n",
      "Training Loss: 0.5433 Validation Loss: 0.5088\n",
      "757/1000:\n",
      "Training Loss: 0.5638 Validation Loss: 0.5093\n",
      "758/1000:\n",
      "Training Loss: 0.5682 Validation Loss: 0.5097\n",
      "759/1000:\n",
      "Training Loss: 0.5473 Validation Loss: 0.5093\n",
      "760/1000:\n",
      "Training Loss: 0.5498 Validation Loss: 0.5082\n",
      "761/1000:\n",
      "Training Loss: 0.5946 Validation Loss: 0.5091\n",
      "762/1000:\n",
      "Training Loss: 0.5655 Validation Loss: 0.5088\n",
      "763/1000:\n",
      "Training Loss: 0.5637 Validation Loss: 0.5082\n",
      "764/1000:\n",
      "Training Loss: 0.5293 Validation Loss: 0.5068\n",
      "765/1000:\n",
      "Training Loss: 0.5852 Validation Loss: 0.5086\n",
      "766/1000:\n",
      "Training Loss: 0.5731 Validation Loss: 0.5086\n",
      "767/1000:\n",
      "Training Loss: 0.5492 Validation Loss: 0.5081\n",
      "768/1000:\n",
      "Training Loss: 0.5366 Validation Loss: 0.5069\n",
      "769/1000:\n",
      "Training Loss: 0.5622 Validation Loss: 0.5074\n",
      "770/1000:\n",
      "Training Loss: 0.5505 Validation Loss: 0.5068\n",
      "771/1000:\n",
      "Training Loss: 0.5660 Validation Loss: 0.5072\n",
      "772/1000:\n",
      "Training Loss: 0.5745 Validation Loss: 0.5078\n",
      "773/1000:\n",
      "Training Loss: 0.5488 Validation Loss: 0.5070\n",
      "774/1000:\n",
      "Training Loss: 0.5523 Validation Loss: 0.5062\n",
      "775/1000:\n",
      "Training Loss: 0.5497 Validation Loss: 0.5057\n",
      "776/1000:\n",
      "Training Loss: 0.5611 Validation Loss: 0.5063\n",
      "777/1000:\n",
      "Training Loss: 0.5604 Validation Loss: 0.5062\n",
      "778/1000:\n",
      "Training Loss: 0.5564 Validation Loss: 0.5063\n",
      "779/1000:\n",
      "Training Loss: 0.5363 Validation Loss: 0.5051\n",
      "780/1000:\n",
      "Training Loss: 0.5677 Validation Loss: 0.5054\n",
      "781/1000:\n",
      "Training Loss: 0.5557 Validation Loss: 0.5053\n",
      "782/1000:\n",
      "Training Loss: 0.5537 Validation Loss: 0.5049\n",
      "783/1000:\n",
      "Training Loss: 0.5605 Validation Loss: 0.5056\n",
      "784/1000:\n",
      "Training Loss: 0.5496 Validation Loss: 0.5049\n",
      "785/1000:\n",
      "Training Loss: 0.5761 Validation Loss: 0.5057\n",
      "786/1000:\n",
      "Training Loss: 0.5658 Validation Loss: 0.5058\n",
      "787/1000:\n",
      "Training Loss: 0.5589 Validation Loss: 0.5062\n",
      "788/1000:\n",
      "Training Loss: 0.5554 Validation Loss: 0.5056\n",
      "789/1000:\n",
      "Training Loss: 0.5585 Validation Loss: 0.5054\n",
      "790/1000:\n",
      "Training Loss: 0.5641 Validation Loss: 0.5051\n",
      "791/1000:\n",
      "Training Loss: 0.5535 Validation Loss: 0.5051\n",
      "792/1000:\n",
      "Training Loss: 0.5512 Validation Loss: 0.5048\n",
      "793/1000:\n",
      "Training Loss: 0.5673 Validation Loss: 0.5051\n",
      "794/1000:\n",
      "Training Loss: 0.5429 Validation Loss: 0.5044\n",
      "795/1000:\n",
      "Training Loss: 0.5487 Validation Loss: 0.5045\n",
      "796/1000:\n",
      "Training Loss: 0.5551 Validation Loss: 0.5041\n",
      "797/1000:\n",
      "Training Loss: 0.5659 Validation Loss: 0.5049\n",
      "798/1000:\n",
      "Training Loss: 0.5527 Validation Loss: 0.5043\n",
      "799/1000:\n",
      "Training Loss: 0.5696 Validation Loss: 0.5043\n",
      "800/1000:\n",
      "Training Loss: 0.5760 Validation Loss: 0.5053\n",
      "801/1000:\n",
      "Training Loss: 0.5562 Validation Loss: 0.5044\n",
      "802/1000:\n",
      "Training Loss: 0.5627 Validation Loss: 0.5049\n",
      "803/1000:\n",
      "Training Loss: 0.5407 Validation Loss: 0.5037\n",
      "804/1000:\n",
      "Training Loss: 0.5859 Validation Loss: 0.5052\n",
      "805/1000:\n",
      "Training Loss: 0.5572 Validation Loss: 0.5045\n",
      "806/1000:\n",
      "Training Loss: 0.5572 Validation Loss: 0.5046\n",
      "807/1000:\n",
      "Training Loss: 0.5667 Validation Loss: 0.5041\n",
      "808/1000:\n",
      "Training Loss: 0.5500 Validation Loss: 0.5045\n",
      "809/1000:\n",
      "Training Loss: 0.5571 Validation Loss: 0.5044\n",
      "810/1000:\n",
      "Training Loss: 0.5538 Validation Loss: 0.5044\n",
      "811/1000:\n",
      "Training Loss: 0.5360 Validation Loss: 0.5033\n",
      "812/1000:\n",
      "Training Loss: 0.5554 Validation Loss: 0.5027\n",
      "813/1000:\n",
      "Training Loss: 0.5669 Validation Loss: 0.5035\n",
      "814/1000:\n",
      "Training Loss: 0.5564 Validation Loss: 0.5035\n",
      "815/1000:\n",
      "Training Loss: 0.5517 Validation Loss: 0.5029\n",
      "816/1000:\n",
      "Training Loss: 0.5579 Validation Loss: 0.5026\n",
      "817/1000:\n",
      "Training Loss: 0.5760 Validation Loss: 0.5033\n",
      "818/1000:\n",
      "Training Loss: 0.5467 Validation Loss: 0.5026\n",
      "819/1000:\n",
      "Training Loss: 0.5497 Validation Loss: 0.5022\n",
      "820/1000:\n",
      "Training Loss: 0.5617 Validation Loss: 0.5024\n",
      "821/1000:\n",
      "Training Loss: 0.5616 Validation Loss: 0.5027\n",
      "822/1000:\n",
      "Training Loss: 0.5522 Validation Loss: 0.5019\n",
      "823/1000:\n",
      "Training Loss: 0.5514 Validation Loss: 0.5016\n",
      "824/1000:\n",
      "Training Loss: 0.5648 Validation Loss: 0.5013\n",
      "825/1000:\n",
      "Training Loss: 0.5681 Validation Loss: 0.5016\n",
      "826/1000:\n",
      "Training Loss: 0.5717 Validation Loss: 0.5023\n",
      "827/1000:\n",
      "Training Loss: 0.5596 Validation Loss: 0.5025\n",
      "828/1000:\n",
      "Training Loss: 0.5588 Validation Loss: 0.5023\n",
      "829/1000:\n",
      "Training Loss: 0.5504 Validation Loss: 0.5025\n",
      "830/1000:\n",
      "Training Loss: 0.5431 Validation Loss: 0.5016\n",
      "831/1000:\n",
      "Training Loss: 0.5554 Validation Loss: 0.5018\n",
      "832/1000:\n",
      "Training Loss: 0.5357 Validation Loss: 0.5006\n",
      "833/1000:\n",
      "Training Loss: 0.5648 Validation Loss: 0.5012\n",
      "834/1000:\n",
      "Training Loss: 0.5511 Validation Loss: 0.5009\n",
      "835/1000:\n",
      "Training Loss: 0.5507 Validation Loss: 0.5003\n",
      "836/1000:\n",
      "Training Loss: 0.5402 Validation Loss: 0.4990\n",
      "837/1000:\n",
      "Training Loss: 0.5518 Validation Loss: 0.4981\n",
      "838/1000:\n",
      "Training Loss: 0.5357 Validation Loss: 0.4970\n",
      "839/1000:\n",
      "Training Loss: 0.5506 Validation Loss: 0.4969\n",
      "840/1000:\n",
      "Training Loss: 0.5428 Validation Loss: 0.4972\n",
      "841/1000:\n",
      "Training Loss: 0.5418 Validation Loss: 0.4971\n",
      "842/1000:\n",
      "Training Loss: 0.5604 Validation Loss: 0.4979\n",
      "843/1000:\n",
      "Training Loss: 0.5588 Validation Loss: 0.4986\n",
      "844/1000:\n",
      "Training Loss: 0.5442 Validation Loss: 0.4979\n",
      "845/1000:\n",
      "Training Loss: 0.5610 Validation Loss: 0.4979\n",
      "846/1000:\n",
      "Training Loss: 0.5550 Validation Loss: 0.4989\n",
      "847/1000:\n",
      "Training Loss: 0.5427 Validation Loss: 0.4979\n",
      "848/1000:\n",
      "Training Loss: 0.5577 Validation Loss: 0.4976\n",
      "849/1000:\n",
      "Training Loss: 0.5732 Validation Loss: 0.4987\n",
      "850/1000:\n",
      "Training Loss: 0.5617 Validation Loss: 0.4994\n",
      "851/1000:\n",
      "Training Loss: 0.5715 Validation Loss: 0.4992\n",
      "852/1000:\n",
      "Training Loss: 0.5546 Validation Loss: 0.4992\n",
      "853/1000:\n",
      "Training Loss: 0.5392 Validation Loss: 0.4984\n",
      "854/1000:\n",
      "Training Loss: 0.5556 Validation Loss: 0.4989\n",
      "855/1000:\n",
      "Training Loss: 0.5442 Validation Loss: 0.4976\n",
      "856/1000:\n",
      "Training Loss: 0.5469 Validation Loss: 0.4982\n",
      "857/1000:\n",
      "Training Loss: 0.5497 Validation Loss: 0.4976\n",
      "858/1000:\n",
      "Training Loss: 0.5502 Validation Loss: 0.4972\n",
      "859/1000:\n",
      "Training Loss: 0.5633 Validation Loss: 0.4974\n",
      "860/1000:\n",
      "Training Loss: 0.5626 Validation Loss: 0.4978\n",
      "861/1000:\n",
      "Training Loss: 0.5665 Validation Loss: 0.4978\n",
      "862/1000:\n",
      "Training Loss: 0.5036 Validation Loss: 0.4957\n",
      "863/1000:\n",
      "Training Loss: 0.5420 Validation Loss: 0.4953\n",
      "864/1000:\n",
      "Training Loss: 0.5723 Validation Loss: 0.4961\n",
      "865/1000:\n",
      "Training Loss: 0.5605 Validation Loss: 0.4971\n",
      "866/1000:\n",
      "Training Loss: 0.5514 Validation Loss: 0.4970\n",
      "867/1000:\n",
      "Training Loss: 0.5666 Validation Loss: 0.4969\n",
      "868/1000:\n",
      "Training Loss: 0.5475 Validation Loss: 0.4967\n",
      "869/1000:\n",
      "Training Loss: 0.5555 Validation Loss: 0.4975\n",
      "870/1000:\n",
      "Training Loss: 0.5692 Validation Loss: 0.4977\n",
      "871/1000:\n",
      "Training Loss: 0.5479 Validation Loss: 0.4968\n",
      "872/1000:\n",
      "Training Loss: 0.5563 Validation Loss: 0.4967\n",
      "873/1000:\n",
      "Training Loss: 0.5291 Validation Loss: 0.4956\n",
      "874/1000:\n",
      "Training Loss: 0.5488 Validation Loss: 0.4955\n",
      "875/1000:\n",
      "Training Loss: 0.5713 Validation Loss: 0.4971\n",
      "876/1000:\n",
      "Training Loss: 0.5557 Validation Loss: 0.4965\n",
      "877/1000:\n",
      "Training Loss: 0.5616 Validation Loss: 0.4968\n",
      "878/1000:\n",
      "Training Loss: 0.5518 Validation Loss: 0.4965\n",
      "879/1000:\n",
      "Training Loss: 0.5624 Validation Loss: 0.4967\n",
      "880/1000:\n",
      "Training Loss: 0.5429 Validation Loss: 0.4964\n",
      "881/1000:\n",
      "Training Loss: 0.5542 Validation Loss: 0.4962\n",
      "882/1000:\n",
      "Training Loss: 0.5496 Validation Loss: 0.4958\n",
      "883/1000:\n",
      "Training Loss: 0.5444 Validation Loss: 0.4953\n",
      "884/1000:\n",
      "Training Loss: 0.5614 Validation Loss: 0.4955\n",
      "885/1000:\n",
      "Training Loss: 0.5529 Validation Loss: 0.4954\n",
      "886/1000:\n",
      "Training Loss: 0.5577 Validation Loss: 0.4952\n",
      "887/1000:\n",
      "Training Loss: 0.5506 Validation Loss: 0.4948\n",
      "888/1000:\n",
      "Training Loss: 0.5375 Validation Loss: 0.4945\n",
      "889/1000:\n",
      "Training Loss: 0.5664 Validation Loss: 0.4948\n",
      "890/1000:\n",
      "Training Loss: 0.5393 Validation Loss: 0.4947\n",
      "891/1000:\n",
      "Training Loss: 0.5617 Validation Loss: 0.4946\n",
      "892/1000:\n",
      "Training Loss: 0.5575 Validation Loss: 0.4944\n",
      "893/1000:\n",
      "Training Loss: 0.5496 Validation Loss: 0.4946\n",
      "894/1000:\n",
      "Training Loss: 0.5582 Validation Loss: 0.4944\n",
      "895/1000:\n",
      "Training Loss: 0.5686 Validation Loss: 0.4955\n",
      "896/1000:\n",
      "Training Loss: 0.5638 Validation Loss: 0.4958\n",
      "897/1000:\n",
      "Training Loss: 0.5467 Validation Loss: 0.4951\n",
      "898/1000:\n",
      "Training Loss: 0.5572 Validation Loss: 0.4952\n",
      "899/1000:\n",
      "Training Loss: 0.5436 Validation Loss: 0.4942\n",
      "900/1000:\n",
      "Training Loss: 0.5427 Validation Loss: 0.4931\n",
      "901/1000:\n",
      "Training Loss: 0.5429 Validation Loss: 0.4927\n",
      "902/1000:\n",
      "Training Loss: 0.5567 Validation Loss: 0.4935\n",
      "903/1000:\n",
      "Training Loss: 0.5289 Validation Loss: 0.4928\n",
      "904/1000:\n",
      "Training Loss: 0.5478 Validation Loss: 0.4926\n",
      "905/1000:\n",
      "Training Loss: 0.5471 Validation Loss: 0.4926\n",
      "906/1000:\n",
      "Training Loss: 0.5459 Validation Loss: 0.4926\n",
      "907/1000:\n",
      "Training Loss: 0.5655 Validation Loss: 0.4926\n",
      "908/1000:\n",
      "Training Loss: 0.5417 Validation Loss: 0.4922\n",
      "909/1000:\n",
      "Training Loss: 0.5581 Validation Loss: 0.4934\n",
      "910/1000:\n",
      "Training Loss: 0.5550 Validation Loss: 0.4933\n",
      "911/1000:\n",
      "Training Loss: 0.5406 Validation Loss: 0.4925\n",
      "912/1000:\n",
      "Training Loss: 0.5510 Validation Loss: 0.4929\n",
      "913/1000:\n",
      "Training Loss: 0.5557 Validation Loss: 0.4935\n",
      "914/1000:\n",
      "Training Loss: 0.5696 Validation Loss: 0.4944\n",
      "915/1000:\n",
      "Training Loss: 0.5496 Validation Loss: 0.4936\n",
      "916/1000:\n",
      "Training Loss: 0.5405 Validation Loss: 0.4927\n",
      "917/1000:\n",
      "Training Loss: 0.5377 Validation Loss: 0.4925\n",
      "918/1000:\n",
      "Training Loss: 0.5392 Validation Loss: 0.4916\n",
      "919/1000:\n",
      "Training Loss: 0.5510 Validation Loss: 0.4917\n",
      "920/1000:\n",
      "Training Loss: 0.5485 Validation Loss: 0.4920\n",
      "921/1000:\n",
      "Training Loss: 0.5575 Validation Loss: 0.4920\n",
      "922/1000:\n",
      "Training Loss: 0.5481 Validation Loss: 0.4921\n",
      "923/1000:\n",
      "Training Loss: 0.5696 Validation Loss: 0.4939\n",
      "924/1000:\n",
      "Training Loss: 0.5431 Validation Loss: 0.4924\n",
      "925/1000:\n",
      "Training Loss: 0.5497 Validation Loss: 0.4931\n",
      "926/1000:\n",
      "Training Loss: 0.5432 Validation Loss: 0.4925\n",
      "927/1000:\n",
      "Training Loss: 0.5446 Validation Loss: 0.4917\n",
      "928/1000:\n",
      "Training Loss: 0.5679 Validation Loss: 0.4918\n",
      "929/1000:\n",
      "Training Loss: 0.5343 Validation Loss: 0.4908\n",
      "930/1000:\n",
      "Training Loss: 0.5331 Validation Loss: 0.4896\n",
      "931/1000:\n",
      "Training Loss: 0.5522 Validation Loss: 0.4900\n",
      "932/1000:\n",
      "Training Loss: 0.5519 Validation Loss: 0.4901\n",
      "933/1000:\n",
      "Training Loss: 0.5799 Validation Loss: 0.4911\n",
      "934/1000:\n",
      "Training Loss: 0.5389 Validation Loss: 0.4905\n",
      "935/1000:\n",
      "Training Loss: 0.5393 Validation Loss: 0.4898\n",
      "936/1000:\n",
      "Training Loss: 0.5395 Validation Loss: 0.4900\n",
      "937/1000:\n",
      "Training Loss: 0.5584 Validation Loss: 0.4902\n",
      "938/1000:\n",
      "Training Loss: 0.5561 Validation Loss: 0.4902\n",
      "939/1000:\n",
      "Training Loss: 0.5615 Validation Loss: 0.4908\n",
      "940/1000:\n",
      "Training Loss: 0.5315 Validation Loss: 0.4896\n",
      "941/1000:\n",
      "Training Loss: 0.5503 Validation Loss: 0.4891\n",
      "942/1000:\n",
      "Training Loss: 0.5423 Validation Loss: 0.4894\n",
      "943/1000:\n",
      "Training Loss: 0.5539 Validation Loss: 0.4897\n",
      "944/1000:\n",
      "Training Loss: 0.5648 Validation Loss: 0.4905\n",
      "945/1000:\n",
      "Training Loss: 0.5492 Validation Loss: 0.4900\n",
      "946/1000:\n",
      "Training Loss: 0.5395 Validation Loss: 0.4894\n",
      "947/1000:\n",
      "Training Loss: 0.5697 Validation Loss: 0.4906\n",
      "948/1000:\n",
      "Training Loss: 0.5535 Validation Loss: 0.4910\n",
      "949/1000:\n",
      "Training Loss: 0.5354 Validation Loss: 0.4905\n",
      "950/1000:\n",
      "Training Loss: 0.5401 Validation Loss: 0.4902\n",
      "951/1000:\n",
      "Training Loss: 0.5355 Validation Loss: 0.4895\n",
      "952/1000:\n",
      "Training Loss: 0.5503 Validation Loss: 0.4896\n",
      "953/1000:\n",
      "Training Loss: 0.5470 Validation Loss: 0.4893\n",
      "954/1000:\n",
      "Training Loss: 0.5611 Validation Loss: 0.4903\n",
      "955/1000:\n",
      "Training Loss: 0.5493 Validation Loss: 0.4902\n",
      "956/1000:\n",
      "Training Loss: 0.5224 Validation Loss: 0.4884\n",
      "957/1000:\n",
      "Training Loss: 0.5638 Validation Loss: 0.4888\n",
      "958/1000:\n",
      "Training Loss: 0.5209 Validation Loss: 0.4880\n",
      "959/1000:\n",
      "Training Loss: 0.5343 Validation Loss: 0.4890\n",
      "960/1000:\n",
      "Training Loss: 0.5702 Validation Loss: 0.4898\n",
      "961/1000:\n",
      "Training Loss: 0.5402 Validation Loss: 0.4888\n",
      "962/1000:\n",
      "Training Loss: 0.5268 Validation Loss: 0.4878\n",
      "963/1000:\n",
      "Training Loss: 0.5346 Validation Loss: 0.4877\n",
      "964/1000:\n",
      "Training Loss: 0.5422 Validation Loss: 0.4878\n",
      "965/1000:\n",
      "Training Loss: 0.5647 Validation Loss: 0.4882\n",
      "966/1000:\n",
      "Training Loss: 0.5435 Validation Loss: 0.4882\n",
      "967/1000:\n",
      "Training Loss: 0.5646 Validation Loss: 0.4889\n",
      "968/1000:\n",
      "Training Loss: 0.5335 Validation Loss: 0.4880\n",
      "969/1000:\n",
      "Training Loss: 0.5440 Validation Loss: 0.4877\n",
      "970/1000:\n",
      "Training Loss: 0.5499 Validation Loss: 0.4869\n",
      "971/1000:\n",
      "Training Loss: 0.5298 Validation Loss: 0.4860\n",
      "972/1000:\n",
      "Training Loss: 0.5332 Validation Loss: 0.4860\n",
      "973/1000:\n",
      "Training Loss: 0.5319 Validation Loss: 0.4857\n",
      "974/1000:\n",
      "Training Loss: 0.5644 Validation Loss: 0.4875\n",
      "975/1000:\n",
      "Training Loss: 0.5289 Validation Loss: 0.4863\n",
      "976/1000:\n",
      "Training Loss: 0.5431 Validation Loss: 0.4873\n",
      "977/1000:\n",
      "Training Loss: 0.5446 Validation Loss: 0.4871\n",
      "978/1000:\n",
      "Training Loss: 0.5308 Validation Loss: 0.4860\n",
      "979/1000:\n",
      "Training Loss: 0.5252 Validation Loss: 0.4849\n",
      "980/1000:\n",
      "Training Loss: 0.5523 Validation Loss: 0.4855\n",
      "981/1000:\n",
      "Training Loss: 0.5551 Validation Loss: 0.4867\n",
      "982/1000:\n",
      "Training Loss: 0.5271 Validation Loss: 0.4857\n",
      "983/1000:\n",
      "Training Loss: 0.5426 Validation Loss: 0.4853\n",
      "984/1000:\n",
      "Training Loss: 0.5388 Validation Loss: 0.4856\n",
      "985/1000:\n",
      "Training Loss: 0.5587 Validation Loss: 0.4865\n",
      "986/1000:\n",
      "Training Loss: 0.5769 Validation Loss: 0.4870\n",
      "987/1000:\n",
      "Training Loss: 0.5394 Validation Loss: 0.4863\n",
      "988/1000:\n",
      "Training Loss: 0.5382 Validation Loss: 0.4855\n",
      "989/1000:\n",
      "Training Loss: 0.5304 Validation Loss: 0.4843\n",
      "990/1000:\n",
      "Training Loss: 0.5417 Validation Loss: 0.4844\n",
      "991/1000:\n",
      "Training Loss: 0.5209 Validation Loss: 0.4842\n",
      "992/1000:\n",
      "Training Loss: 0.5627 Validation Loss: 0.4853\n",
      "993/1000:\n",
      "Training Loss: 0.5435 Validation Loss: 0.4857\n",
      "994/1000:\n",
      "Training Loss: 0.5607 Validation Loss: 0.4862\n",
      "995/1000:\n",
      "Training Loss: 0.5265 Validation Loss: 0.4855\n",
      "996/1000:\n",
      "Training Loss: 0.5267 Validation Loss: 0.4852\n",
      "997/1000:\n",
      "Training Loss: 0.5574 Validation Loss: 0.4855\n",
      "998/1000:\n",
      "Training Loss: 0.5449 Validation Loss: 0.4854\n",
      "999/1000:\n",
      "Training Loss: 0.5391 Validation Loss: 0.4849\n",
      "1000/1000:\n",
      "Training Loss: 0.5469 Validation Loss: 0.4850\n"
     ]
    }
   ],
   "source": [
    "second_model = second_model.fit(X_train, y_train, X_test, y_test, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac2e576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:42.543757400Z",
     "start_time": "2023-08-01T13:40:42.512777300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.7892376681614349"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = second_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96a6b878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:42.590284Z",
     "start_time": "2023-08-01T13:40:42.527635700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [0.],\n        [1.],\n        [0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.],\n        [1.],\n        [0.],\n        [0.],\n        [1.],\n        [0.]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03be9ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:40:42.658639600Z",
     "start_time": "2023-08-01T13:40:42.556364700Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(second_model, 'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3ebb481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T13:41:30.524558300Z",
     "start_time": "2023-08-01T13:41:30.268540300Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43msecond_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmy_model.pth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deeplearningvm\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1994\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1971\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001B[39;00m\n\u001B[0;32m   1972\u001B[0m \u001B[38;5;124;03mthis module and its descendants. If :attr:`strict` is ``True``, then\u001B[39;00m\n\u001B[0;32m   1973\u001B[0m \u001B[38;5;124;03mthe keys of :attr:`state_dict` must exactly match the keys returned\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1991\u001B[0m \u001B[38;5;124;03m    ``RuntimeError``.\u001B[39;00m\n\u001B[0;32m   1992\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1993\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(state_dict, Mapping):\n\u001B[1;32m-> 1994\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected state_dict to be dict-like, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(state_dict)))\n\u001B[0;32m   1996\u001B[0m missing_keys: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   1997\u001B[0m unexpected_keys: List[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mTypeError\u001B[0m: Expected state_dict to be dict-like, got <class 'str'>."
     ]
    }
   ],
   "source": [
    "#second_model.load_state_dict(state_dict='my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b443e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-01T13:40:42.618022300Z"
    }
   },
   "outputs": [],
   "source": [
    "#torch.load('my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931eb03",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-01T13:40:08.324295200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
