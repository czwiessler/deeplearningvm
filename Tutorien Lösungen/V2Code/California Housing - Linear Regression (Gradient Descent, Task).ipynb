{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation and application of a multivariate linear regression model for the \"housing\" dataset. You can download the dataset from http://lib.stat.cmu.edu/datasets/.\n",
    "\n",
    "**Comments:** \"*The file cadata.txt contains all the the variables. Specifically, it contains median house value, median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude in that order. The dependent variable is ln(median house value).*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# load data from text file (note that the initial comments have been deleted from the original file)\n",
    "data = numpy.loadtxt(\"cadata.txt\")\n",
    "\n",
    "# the first column corresponds to the target variables; the remaining ones are the features\n",
    "y, X = data[:,0], data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a class for our linear regression model that implements the \"fitting\" phase (computation of the weights) and the \"prediction\" phase (computation of predictions for new data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \"\"\"\n",
    "    Linear regression implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, algorithm=\"solve\", lr=0.01, maxiter=1000):\n",
    "        \"\"\"\n",
    "        Instantiates the model object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : str, default=\"solve\"\n",
    "            The type of algorithm that shall be applied.\n",
    "        lr : float, default 0.02\n",
    "            The learning rate for gradient descent.\n",
    "        maxiter : int, default 100\n",
    "            The maximal number of iterations for gradient descent\n",
    "        \"\"\"           \n",
    "        \n",
    "        self.algorithm = algorithm\n",
    "        self.lr = lr\n",
    "        self.maxiter = maxiter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the linear regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "        y : Array of shape [n_samples, 1]\n",
    "        \"\"\"        \n",
    "\n",
    "        # make sure that we have multidimensional numpy arrays\n",
    "        X = numpy.array(X).reshape((X.shape[0], -1))\n",
    "        # IMPORTANT: Make sure that we have a column vector! \n",
    "        y = numpy.array(y).reshape((len(y), 1))\n",
    "\n",
    "        # prepend a column of ones\n",
    "        ones = numpy.ones((X.shape[0], 1))\n",
    "        X = numpy.concatenate((ones, X), axis=1)           \n",
    "\n",
    "        # compute weights\n",
    "        if self.algorithm == \"solve\":\n",
    "            \n",
    "            XtX_pinv = numpy.linalg.pinv(numpy.dot(X.T, X))\n",
    "            Xty = numpy.dot(X.T,y)\n",
    "            self._w = numpy.dot(XtX_pinv, Xty)\n",
    "            \n",
    "        elif self.algorithm == \"gradient\":\n",
    "            \n",
    "            # initialize with zeros (alternatively: small random values)\n",
    "            self._w = numpy.zeros((X.shape[1], 1))\n",
    "            \n",
    "            # TODO: YOUR CODE HERE\n",
    "                            \n",
    "        else:\n",
    "            \n",
    "            raise Exception(\"Algorithm {} not implemented!\".format(self.algorithm))\n",
    "                \n",
    "    def predict(self, X, add_ones=True):\n",
    "        \"\"\"\n",
    "        Computes predictions for a new set of points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Array of shape [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Array of shape [n_samples, 1]\n",
    "        \"\"\"                     \n",
    "\n",
    "        # make sure that we have multidimensional numpy arrays\n",
    "        X = numpy.array(X).reshape((X.shape[0], -1))\n",
    "\n",
    "        if add_ones == True:\n",
    "            # prepend a column of ones\n",
    "            ones = numpy.ones((X.shape[0], 1))\n",
    "            X = numpy.concatenate((ones, X), axis=1)           \n",
    "\n",
    "        # compute predictions\n",
    "        predictions = numpy.dot(X, self._w)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us normalize the data first. This makes it easier for gradient descent to find the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first instantiate the \"model\" object. Afterwards, we call the \"fit\" method to fit our model (i.e., to compute the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(algorithm=\"gradient\", lr=0.1, maxiter=1000)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fitted model, we can now obtain predictions for new data points. For simplification, we just use our data points again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a look at the quality of our model by computing the RMSE and by generating a plot \"predictions\" vs. \"true labels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute RMSE\n",
    "print(\"RMSE: {}\".format(numpy.sqrt(mean_squared_error(y, preds))))\n",
    "\n",
    "# visualize predictions vs. true labels\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(preds, y, color=\"blue\", alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "plt.plot([-100000,600000], [-100000, 600000], 'k--')\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xlim([-100000,600000])\n",
    "plt.ylim([-100000,600000])\n",
    "plt.title(\"Evaluation of Regression Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
